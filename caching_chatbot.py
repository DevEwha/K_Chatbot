from vllm import LLM, SamplingParams
import time

# 1. vLLM ì—”ì§„ ì´ˆê¸°í™” (ì—¬ê¸°ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  GPU ë©”ëª¨ë¦¬ë¥¼ ì˜ˆì•½í•©ë‹ˆë‹¤)
# gpu_memory_utilization: GPU ë©”ëª¨ë¦¬ë¥¼ ì–¼ë§ˆë‚˜ ì“¸ì§€ ì„¤ì • (ê¸°ë³¸ê°’ 0.9)
# prefix_cache_num_layers: ì²˜ìŒ Nê°œ ë ˆì´ì–´ë§Œ prefix caching ì‚¬ìš© (ë‚˜ë¨¸ì§€ëŠ” ë§¤ë²ˆ ì¬ê³„ì‚°)
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    gpu_memory_utilization=0.5,
    enable_prefix_caching=True,
    prefix_cache_num_layers=15  # ë ˆì´ì–´ 0-14ë§Œ prefix caching, 15-31ì€ ë§¤ë²ˆ ì¬ê³„ì‚°
)

# 2. í…ìŠ¤íŠ¸ ìƒì„± ì˜µì…˜ ì„¤ì • (ì˜¨ë„, ìµœëŒ€ ê¸¸ì´ ë“±)
sampling_params = SamplingParams(temperature=0.7, max_tokens=200)

# 3. ëŒ€í™” ë¬¸ë§¥ì„ ì €ì¥í•  ë³€ìˆ˜
history_prompt = ""

print("vLLM ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. (ì¢…ë£Œ: 'quit')")
print("-" * 30)

while True:
    user_input = input("\nUser: ")
    if user_input.lower() == "quit":
        break

    # KV cache ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    if user_input.lower() == "reset":
        print("ğŸ”„ KV cacheë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        start = time.time()
        llm.sleep(level=1)  # KV cache ì‚­ì œ
        llm.wake_up()       # ì—”ì§„ ì¬ì‹œì‘
        print(f"âœ… ì´ˆê¸°í™” ì™„ë£Œ ({time.time() - start:.2f}s)")
        #history_prompt = ""  # ëŒ€í™” íˆìŠ¤í† ë¦¬ë„ ë¦¬ì…‹
        continue
        
    # 4. Llama 2 í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì ìš©
    # ì´ì „ ëŒ€í™”(history_prompt) ë’¤ì— ìƒˆ ì§ˆë¬¸ì„ ë¶™ì…ë‹ˆë‹¤.
    current_prompt = history_prompt + f"[INST] {user_input} [/INST]"

    # ìƒì„± ì‹œê°„ ì¸¡ì • (prefix caching íš¨ê³¼ í™•ì¸ìš©)
    start = time.time()

    # 5. vLLM ìƒì„± (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤)
    # vLLMì€ ë‚´ë¶€ì ìœ¼ë¡œ ì•ë¶€ë¶„(history_prompt)ì´ ì´ì „ê³¼ ê°™ë‹¤ëŠ” ê²ƒì„ ê°ì§€í•˜ê³ 
    # ìë™ìœ¼ë¡œ KV Cacheë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ê³„ì‚° ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
    outputs = llm.generate([current_prompt], sampling_params)

    elapsed = time.time() - start

    # 6. ê²°ê³¼ ì¶”ì¶œ
    # generated_textì—ëŠ” ì…ë ¥ í”„ë¡¬í”„íŠ¸ëŠ” ì œì™¸í•˜ê³  'ìƒˆë¡œ ìƒì„±ëœ ë‹µë³€'ë§Œ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
    response = outputs[0].outputs[0].text
    
    print(f"Bot: {response}")
    print(f"â±ï¸  ìƒì„± ì‹œê°„: {elapsed:.2f}s")

    # 7. ë¬¸ë§¥ ì—…ë°ì´íŠ¸ (ë‹µë³€ë„ ê¸°ì–µì— ì¶”ê°€)
    history_prompt = current_prompt + f" {response} "