"""
Split KV Caching for Progressive LLM Serving
progressive_serve/split_kv_cache.py

í•µì‹¬ ì•„ì´ë””ì–´:
- KV Cacheë¥¼ Base (ë¶ˆë³€)ì™€ Delta (ê°€ë³€)ë¡œ ë¶„ë¦¬
- Stage ì „í™˜ ì‹œ BaseëŠ” ì¬ì‚¬ìš©, Deltaë§Œ ì¬ê³„ì‚°
- ë ˆì´ì–´ë³„ ì„ íƒì  ì¬ì‚¬ìš©ìœ¼ë¡œ Prefill ì‹œê°„ 75-90% ê°ì†Œ
"""

from dataclasses import dataclass, field
from typing import Optional, Dict, List, Tuple, Set
import torch


@dataclass
class SplitKVCache:
    """
    ë‹¨ì¼ ë ˆì´ì–´ë¥¼ ìœ„í•œ Split KV Cache
    
    êµ¬ì¡°:
        K_final = K_base + K_delta
        V_final = V_base + V_delta
        
    - K_base, V_base: Base ëª¨ë¸ì˜ KV (stage ì „í™˜ ì‹œì—ë„ ìœ ì§€)
    - K_delta, V_delta: LoRA ì–´ëŒ‘í„°ì˜ delta (stage ë³€ê²½ ì‹œ ì¬ê³„ì‚°)
    """
    
    # ì˜êµ¬ ì»´í¬ë„ŒíŠ¸ (stage ì „í™˜ ì‹œì—ë„ ìœ ì§€)
    k_base: torch.Tensor  # [batch, seq_len, num_heads, head_dim] or vLLM format
    v_base: torch.Tensor
    
    # ì„ì‹œ ì»´í¬ë„ŒíŠ¸ (stage ë³€ê²½ ì‹œ ì¬ê³„ì‚°)
    k_delta: Optional[torch.Tensor] = None
    v_delta: Optional[torch.Tensor] = None
    
    # ë©”íƒ€ë°ì´í„°
    layer_idx: int = 0
    seq_len: int = 0
    
    # ì…ë ¥ hidden statesì˜ í•´ì‹œ (ë””ë²„ê¹…ìš©)
    input_hash: Optional[int] = None
    
    def get_final_kv(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """Baseì™€ Deltaë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… KV ë°˜í™˜"""
        if self.k_delta is None:
            return self.k_base, self.v_base
        return (self.k_base + self.k_delta, 
                self.v_base + self.v_delta)
    
    def update_delta(self, k_delta: torch.Tensor, v_delta: torch.Tensor):
        """Delta ì—…ë°ì´íŠ¸ (ì–´ëŒ‘í„° êµì²´ ì‹œ)"""
        self.k_delta = k_delta
        self.v_delta = v_delta
    
    def clear_delta(self):
        """Delta ì œê±°"""
        self.k_delta = None
        self.v_delta = None
    
    def memory_usage_mb(self) -> Tuple[float, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚° (MB)"""
        base_mem = (self.k_base.numel() + self.v_base.numel()) * self.k_base.element_size() / 1024 / 1024
        delta_mem = 0.0
        if self.k_delta is not None:
            delta_mem = (self.k_delta.numel() + self.v_delta.numel()) * self.k_delta.element_size() / 1024 / 1024
        return base_mem, delta_mem
    
    def to(self, device: torch.device) -> 'SplitKVCache':
        """ë””ë°”ì´ìŠ¤ ì´ë™"""
        self.k_base = self.k_base.to(device)
        self.v_base = self.v_base.to(device)
        if self.k_delta is not None:
            self.k_delta = self.k_delta.to(device)
            self.v_delta = self.v_delta.to(device)
        return self


class SplitCacheManager:
    """
    ì „ì²´ ë ˆì´ì–´ì˜ Split KV Cache ê´€ë¦¬
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. Cache ì €ì¥ ë° ì¡°íšŒ
    2. Delta ë¬´íš¨í™” (ì–´ëŒ‘í„° êµì²´ ì‹œ)
    3. ë ˆì´ì–´ ë²”ìœ„ ë¬´íš¨í™” (ì…ë ¥ ë³€ê²½ ì‹œ)
    4. Stage ì „í™˜ ë¶„ì„ ë° ìë™ ê´€ë¦¬
    """
    
    def __init__(self, num_layers: int):
        """
        Args:
            num_layers: ëª¨ë¸ì˜ ì´ ë ˆì´ì–´ ìˆ˜ (e.g., 32 for Llama-7B)
        """
        self.caches: Dict[int, SplitKVCache] = {}
        self.num_layers = num_layers
        
        # Stage ì„¤ì • (Progressive Serving êµ¬ì¡°)
        # ì‹¤ì œ ì‚¬ìš© ì‹œ prune_log.jsonì—ì„œ ì½ì–´ì˜´
        self.stage_configs: Dict[int, Dict] = {}
        self.current_stage: int = 1
        
        # í†µê³„
        self.stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'delta_recomputes': 0,
        }
    
    def set_stage_configs(
        self, 
        stage_configs: Dict[int, Dict],
        current_stage: int = 1
    ):
        """
        Stage ì„¤ì • ì €ì¥
        
        Args:
            stage_configs: Stageë³„ ë ˆì´ì–´ êµ¬ì„±
                ì˜ˆì‹œ:
                {
                    1: {'active_layers': [(0, 20), (29, 31)]},
                    2: {'active_layers': [(0, 20), (21, 24), (29, 31)]},
                    3: {'active_layers': [(0, 31)]},
                }
            current_stage: í˜„ì¬ stage
        """
        self.stage_configs = stage_configs
        self.current_stage = current_stage
    
    # ============================================================
    # Cache ì €ì¥ ë° ì¡°íšŒ
    # ============================================================
    
    def has_cache(self, layer_idx: int) -> bool:
        """í•´ë‹¹ ë ˆì´ì–´ì˜ cache ì¡´ì¬ ì—¬ë¶€"""
        return layer_idx in self.caches
    
    def get_cache(self, layer_idx: int) -> Optional[SplitKVCache]:
        """í•´ë‹¹ ë ˆì´ì–´ì˜ cache ë°˜í™˜"""
        return self.caches.get(layer_idx, None)
    
    def set_cache(
        self, 
        layer_idx: int, 
        k_base: torch.Tensor, 
        v_base: torch.Tensor,
        k_delta: Optional[torch.Tensor] = None,
        v_delta: Optional[torch.Tensor] = None,
        seq_len: int = 0,
        input_hash: Optional[int] = None,
    ):
        """
        Cache ì €ì¥
        
        Args:
            layer_idx: ë ˆì´ì–´ ì¸ë±ìŠ¤
            k_base, v_base: Base KV (detachëœ í…ì„œ)
            k_delta, v_delta: Delta KV (optional, detachëœ í…ì„œ)
            seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
            input_hash: ì…ë ¥ hidden states í•´ì‹œ (ë””ë²„ê¹…ìš©)
        """
        self.caches[layer_idx] = SplitKVCache(
            k_base=k_base.detach() if k_base.requires_grad else k_base,
            v_base=v_base.detach() if v_base.requires_grad else v_base,
            k_delta=k_delta.detach() if k_delta is not None and k_delta.requires_grad else k_delta,
            v_delta=v_delta.detach() if v_delta is not None and v_delta.requires_grad else v_delta,
            layer_idx=layer_idx,
            seq_len=seq_len,
            input_hash=input_hash,
        )
    
    def append_to_cache(
        self,
        layer_idx: int,
        k_base_new: torch.Tensor,
        v_base_new: torch.Tensor,
        k_delta_new: Optional[torch.Tensor] = None,
        v_delta_new: Optional[torch.Tensor] = None,
    ):
        """
        ê¸°ì¡´ cacheì— ìƒˆ í† í° ì¶”ê°€ (decode phase)
        
        Args:
            layer_idx: ë ˆì´ì–´ ì¸ë±ìŠ¤
            k_base_new, v_base_new: ìƒˆ í† í°ì˜ Base KV
            k_delta_new, v_delta_new: ìƒˆ í† í°ì˜ Delta KV (optional)
        """
        if layer_idx not in self.caches:
            raise ValueError(f"No cache for layer {layer_idx}")
        
        cache = self.caches[layer_idx]
        
        # Base ì—°ê²°
        cache.k_base = torch.cat([cache.k_base, k_base_new.detach()], dim=1)
        cache.v_base = torch.cat([cache.v_base, v_base_new.detach()], dim=1)
        
        # Delta ì—°ê²° (ìˆëŠ” ê²½ìš°)
        if cache.k_delta is not None and k_delta_new is not None:
            cache.k_delta = torch.cat([cache.k_delta, k_delta_new.detach()], dim=1)
            cache.v_delta = torch.cat([cache.v_delta, v_delta_new.detach()], dim=1)
        
        cache.seq_len += k_base_new.size(1)
    
    # ============================================================
    # ë¬´íš¨í™” ë©”ì„œë“œ
    # ============================================================
    
    def invalidate_deltas(self):
        """
        ëª¨ë“  delta cache ì œê±° (ì–´ëŒ‘í„° êµì²´ ì‹œ)
        
        ì´ìœ : LoRA ì–´ëŒ‘í„°ê°€ ë°”ë€Œë©´ Deltaê°€ ë‹¬ë¼ì§
        """
        print("ğŸ§¹ Delta cache ë¬´íš¨í™” ì¤‘...")
        count = 0
        for cache in self.caches.values():
            if cache.k_delta is not None:
                cache.clear_delta()
                count += 1
        print(f"  âœ… {count}ê°œ ë ˆì´ì–´ì˜ delta cache ì‚­ì œ")
        self.stats['delta_recomputes'] += count
    
    def invalidate_layer_range(self, start: int, end: int):
        """
        ì…ë ¥ì´ ë³€ê²½ëœ ë ˆì´ì–´ì˜ cache ì „ì²´ ì œê±°
        
        Args:
            start: ì‹œì‘ ë ˆì´ì–´ ì¸ë±ìŠ¤ (inclusive)
            end: ë ë ˆì´ì–´ ì¸ë±ìŠ¤ (inclusive)
            
        ì´ìœ : ì¤‘ê°„ì— ë ˆì´ì–´ê°€ ì¶”ê°€ë˜ë©´ ì´í›„ ë ˆì´ì–´ë“¤ì˜ ì…ë ¥ì´ ë³€ê²½ë¨
        """
        print(f"ğŸ§¹ Layer {start}-{end} cache ë¬´íš¨í™” ì¤‘...")
        count = 0
        for layer_idx in range(start, end + 1):
            if layer_idx in self.caches:
                del self.caches[layer_idx]
                count += 1
        print(f"  âœ… {count}ê°œ ë ˆì´ì–´ì˜ cache ì‚­ì œ")
    
    def invalidate_all(self):
        """ëª¨ë“  cache ì œê±°"""
        print("ğŸ§¹ ì „ì²´ cache ë¬´íš¨í™”...")
        count = len(self.caches)
        self.caches.clear()
        print(f"  âœ… {count}ê°œ ë ˆì´ì–´ì˜ cache ì‚­ì œ")
    
    # ============================================================
    # Stage ì „í™˜ ë¶„ì„
    # ============================================================
    
    def analyze_stage_transition(
        self, 
        current_stage: int, 
        next_stage: int
    ) -> Dict:
        """
        Stage ì „í™˜ ì‹œ ë ˆì´ì–´ ë³€í™” ë¶„ì„
        
        Args:
            current_stage: í˜„ì¬ stage
            next_stage: ë‹¤ìŒ stage
            
        Returns:
            {
                'reusable': [(start, end), ...],      # ì¬ì‚¬ìš© ê°€ëŠ¥ ë²”ìœ„
                'new': [(start, end), ...],           # ìƒˆë¡œ ì¶”ê°€ëœ ë²”ìœ„
                'invalidated': [(start, end), ...],   # ë¬´íš¨í™” í•„ìš” ë²”ìœ„
            }
        """
        if not self.stage_configs:
            print("âš ï¸  Stage configs not set. Using default analysis.")
            return self._analyze_default(current_stage, next_stage)
        
        current_config = self.stage_configs.get(current_stage, {})
        next_config = self.stage_configs.get(next_stage, {})
        
        current_ranges = current_config.get('active_layers', [])
        next_ranges = next_config.get('active_layers', [])
        
        return self._analyze_layer_changes(current_ranges, next_ranges)
    
    def _analyze_layer_changes(
        self, 
        current_ranges: List[Tuple[int, int]], 
        next_ranges: List[Tuple[int, int]]
    ) -> Dict:
        """
        ë ˆì´ì–´ ë³€í™” ë¶„ì„ (ë‚´ë¶€ ë©”ì„œë“œ)
        
        í•µì‹¬ ë¡œì§:
        - ìƒˆ ë ˆì´ì–´ê°€ ì¶”ê°€ë˜ëŠ” ìœ„ì¹˜ë¥¼ ì°¾ìŒ
        - ê·¸ ìœ„ì¹˜ ì´ì „ì˜ ë ˆì´ì–´ë“¤ì€ ì…ë ¥ì´ ë™ì¼í•˜ë¯€ë¡œ ì¬ì‚¬ìš© ê°€ëŠ¥
        - ê·¸ ìœ„ì¹˜ ì´í›„ì˜ ë ˆì´ì–´ë“¤ì€ ì…ë ¥ì´ ë³€ê²½ë˜ë¯€ë¡œ ë¬´íš¨í™” í•„ìš”
        
        ì˜ˆì‹œ (Stage 1 â†’ 2):
        - Stage 1: Layer 0-20, 29-31
        - Stage 2: Layer 0-20, 21-24, 29-31
        - ìƒˆ ë ˆì´ì–´ 21-24ê°€ Layer 20 ë’¤ì— ì¶”ê°€ë¨
        - Layer 0-20: ì…ë ¥ ë™ì¼ â†’ ì¬ì‚¬ìš© ê°€ëŠ¥
        - Layer 29-31: ì…ë ¥ ë³€ê²½ (hidden_20 â†’ hidden_24) â†’ ë¬´íš¨í™”
        """
        current_set = self._ranges_to_set(current_ranges)
        next_set = self._ranges_to_set(next_ranges)
        
        # 1. ìƒˆë¡œ ì¶”ê°€ëœ ë ˆì´ì–´ ì°¾ê¸°
        new_layers = sorted(next_set - current_set)
        new = self._set_to_ranges(new_layers)
        
        # 2. ìƒˆ ë ˆì´ì–´ê°€ ì¶”ê°€ë˜ëŠ” ì²« ë²ˆì§¸ ìœ„ì¹˜ ì°¾ê¸°
        # ì´ ìœ„ì¹˜ ì´ì „ì˜ ë ˆì´ì–´ë“¤ì€ ì¬ì‚¬ìš© ê°€ëŠ¥
        if new_layers:
            first_new_layer = min(new_layers)
        else:
            # ìƒˆ ë ˆì´ì–´ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ê¸°ì¡´ ë ˆì´ì–´ ì¬ì‚¬ìš© ê°€ëŠ¥
            first_new_layer = float('inf')
        
        # 3. ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë ˆì´ì–´: ì²« ë²ˆì§¸ ìƒˆ ë ˆì´ì–´ ì´ì „ì˜ ëª¨ë“  ê¸°ì¡´ ë ˆì´ì–´
        reusable_layers = [
            l for l in current_set 
            if l < first_new_layer and l in next_set
        ]
        reusable = self._set_to_ranges(reusable_layers)
        
        # 4. ë¬´íš¨í™” í•„ìš”í•œ ë ˆì´ì–´: ì²« ë²ˆì§¸ ìƒˆ ë ˆì´ì–´ ì´í›„ì˜ ê¸°ì¡´ ë ˆì´ì–´
        # (ë‹¤ìŒ stageì—ë„ ì¡´ì¬í•˜ì§€ë§Œ ì…ë ¥ì´ ë³€ê²½ë¨)
        invalidated_layers = [
            l for l in current_set 
            if l >= first_new_layer and l in next_set
        ]
        invalidated = self._set_to_ranges(invalidated_layers)
        
        return {
            'reusable': reusable,
            'new': new,
            'invalidated': invalidated,
        }
    
    def _analyze_default(self, current_stage: int, next_stage: int) -> Dict:
        """
        ê¸°ë³¸ ë¶„ì„ (Progressive Serving ê¸°ì¤€)
        
        Stage 1: Layer 0-20, 29-31
        Stage 2: Layer 0-20, 21-24, 29-31
        Stage 3: Layer 0-31
        """
        if current_stage == 1 and next_stage == 2:
            return {
                'reusable': [(0, 20)],
                'new': [(21, 24)],
                'invalidated': [(29, 31)],
            }
        elif current_stage == 2 and next_stage == 3:
            return {
                'reusable': [(0, 20), (21, 24)],
                'new': [(25, 28)],
                'invalidated': [(29, 31)],
            }
        else:
            return {
                'reusable': [],
                'new': [],
                'invalidated': [],
            }
    
    # ============================================================
    # Stage ì „í™˜ ì‹¤í–‰
    # ============================================================
    
    def handle_stage_transition(
        self, 
        current_stage: int, 
        next_stage: int,
        verbose: bool = True
    ) -> Dict:
        """
        Stage ì „í™˜ ì‹œ cache ê´€ë¦¬
        
        Args:
            current_stage: í˜„ì¬ stage
            next_stage: ë‹¤ìŒ stage
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
            
        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ”„ Stage ì „í™˜: {current_stage} â†’ {next_stage}")
            print(f"{'='*60}\n")
        
        # 1. ë ˆì´ì–´ êµ¬ì¡° ë¶„ì„
        analysis = self.analyze_stage_transition(current_stage, next_stage)
        
        if verbose:
            print(f"ğŸ“‹ ë ˆì´ì–´ êµ¬ì¡° ë¶„ì„:")
            if analysis['reusable']:
                print(f"  âœ… ì¬ì‚¬ìš© ê°€ëŠ¥: {self._format_ranges(analysis['reusable'])}")
            if analysis['invalidated']:
                print(f"  âŒ ë¬´íš¨í™” í•„ìš”: {self._format_ranges(analysis['invalidated'])}")
            if analysis['new']:
                print(f"  ğŸ†• ìƒˆë¡œ ì¶”ê°€: {self._format_ranges(analysis['new'])}")
            print()
        
        # 2. Delta cache ë¬´íš¨í™” (ì–´ëŒ‘í„° êµì²´)
        self.invalidate_deltas()
        
        # 3. ì˜í–¥ë°›ì€ ë ˆì´ì–´ì˜ Base cacheë„ ë¬´íš¨í™” (ì…ë ¥ ë³€ê²½)
        for start, end in analysis['invalidated']:
            self.invalidate_layer_range(start, end)
        
        # 4. Stage ì—…ë°ì´íŠ¸
        self.current_stage = next_stage
        
        if verbose:
            stats = self.get_memory_stats()
            print(f"\nğŸ’¾ Cache ë©”ëª¨ë¦¬ í†µê³„:")
            print(f"  ìºì‹œëœ ë ˆì´ì–´ ìˆ˜: {stats['num_layers']}")
            print(f"  Base cache: {stats['base_MB']:.2f} MB (ìœ ì§€)")
            print(f"  Delta cache: {stats['delta_MB']:.2f} MB")
            print(f"  ì´ ë©”ëª¨ë¦¬: {stats['total_MB']:.2f} MB")
            print(f"\nâœ… Stage {next_stage} ì „í™˜ ì¤€ë¹„ ì™„ë£Œ!")
            print(f"{'='*60}\n")
        
        return analysis
    
    # ============================================================
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
    # ============================================================
    
    def _ranges_to_set(self, ranges: List[Tuple[int, int]]) -> Set[int]:
        """Range listë¥¼ setìœ¼ë¡œ ë³€í™˜"""
        result = set()
        for start, end in ranges:
            result.update(range(start, end + 1))
        return result
    
    def _set_to_ranges(self, layer_set: Set[int]) -> List[Tuple[int, int]]:
        """Setì„ ì—°ì†ëœ range listë¡œ ë³€í™˜"""
        if not layer_set:
            return []
        
        sorted_layers = sorted(layer_set)
        ranges = []
        start = sorted_layers[0]
        prev = start
        
        for curr in sorted_layers[1:]:
            if curr != prev + 1:
                ranges.append((start, prev))
                start = curr
            prev = curr
        
        ranges.append((start, prev))
        return ranges
    
    def _format_ranges(self, ranges: List[Tuple[int, int]]) -> str:
        """Rangeë¥¼ ì½ê¸° ì‰¬ìš´ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if not ranges:
            return "ì—†ìŒ"
        parts = []
        for start, end in ranges:
            if start == end:
                parts.append(f"Layer {start}")
            else:
                parts.append(f"Layer {start}-{end}")
        return ", ".join(parts)
    
    # ============================================================
    # í†µê³„ ë° ë””ë²„ê¹…
    # ============================================================
    
    def get_memory_stats(self) -> Dict:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© í†µê³„"""
        total_base = 0.0
        total_delta = 0.0
        
        for cache in self.caches.values():
            base, delta = cache.memory_usage_mb()
            total_base += base
            total_delta += delta
        
        return {
            'base_MB': total_base,
            'delta_MB': total_delta,
            'total_MB': total_base + total_delta,
            'overhead_pct': (total_delta / total_base * 100) if total_base > 0 else 0,
            'num_layers': len(self.caches),
        }
    
    def print_status(self):
        """í˜„ì¬ cache ìƒíƒœ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"Split Cache Manager Status - Stage {self.current_stage}")
        print(f"{'='*60}")
        
        if not self.caches:
            print("  (No cached layers)")
        else:
            # Layer ê·¸ë£¹ë³„ë¡œ í‘œì‹œ
            groups = {
                '0-20': range(0, 21),
                '21-24': range(21, 25),
                '25-28': range(25, 29),
                '29-31': range(29, 32)
            }
            
            for group_name, layer_range in groups.items():
                cached_in_group = [l for l in layer_range if l in self.caches]
                if cached_in_group:
                    print(f"\nLayer {group_name}:")
                    for layer_idx in cached_in_group:
                        cache = self.caches[layer_idx]
                        base_mb, delta_mb = cache.memory_usage_mb()
                        
                        status = "âœ… Base+Delta" if cache.k_delta is not None else "âš ï¸  Base only"
                        print(f"  L{layer_idx:2d}: {status} | Base: {base_mb:.1f}MB | Delta: {delta_mb:.1f}MB | Seq: {cache.seq_len}")
        
        stats = self.get_memory_stats()
        print(f"\nì´ ë©”ëª¨ë¦¬: {stats['total_MB']:.1f}MB across {stats['num_layers']} layers")
        print(f"í†µê³„: hits={self.stats['cache_hits']}, misses={self.stats['cache_misses']}, delta_recomputes={self.stats['delta_recomputes']}")
        print(f"{'='*60}\n")
    
    def get_cached_layers(self) -> List[int]:
        """ìºì‹œëœ ë ˆì´ì–´ ì¸ë±ìŠ¤ ëª©ë¡"""
        return sorted(self.caches.keys())
    
    def is_layer_reusable(self, layer_idx: int, next_stage: int) -> bool:
        """
        íŠ¹ì • ë ˆì´ì–´ê°€ ë‹¤ìŒ stageì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        
        Args:
            layer_idx: ë ˆì´ì–´ ì¸ë±ìŠ¤
            next_stage: ë‹¤ìŒ stage
            
        Returns:
            ì¬ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
        """
        if layer_idx not in self.caches:
            return False
        
        analysis = self.analyze_stage_transition(self.current_stage, next_stage)
        
        for start, end in analysis['reusable']:
            if start <= layer_idx <= end:
                return True
        
        return False


# ============================================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================================

if __name__ == "__main__":
    print("Split KV Cache Manager Test")
    print("="*60)
    
    # Manager ìƒì„±
    manager = SplitCacheManager(num_layers=32)
    
    # Stage ì„¤ì • (Progressive Serving êµ¬ì¡°)
    stage_configs = {
        1: {'active_layers': [(0, 20), (29, 31)]},
        2: {'active_layers': [(0, 20), (21, 24), (29, 31)]},
        3: {'active_layers': [(0, 31)]},
    }
    manager.set_stage_configs(stage_configs, current_stage=1)
    
    # ë”ë¯¸ cache ìƒì„±
    print("\n1. ë”ë¯¸ cache ìƒì„± (Stage 1)")
    for layer_idx in [0, 5, 10, 15, 20, 29, 30, 31]:
        k_base = torch.randn(1, 100, 32, 128)  # [batch, seq, heads, dim]
        v_base = torch.randn(1, 100, 32, 128)
        manager.set_cache(layer_idx, k_base, v_base, seq_len=100)
    
    manager.print_status()
    
    # Stage 1 â†’ 2 ì „í™˜ ë¶„ì„
    print("\n2. Stage 1 â†’ 2 ì „í™˜")
    analysis = manager.handle_stage_transition(1, 2)
    
    manager.print_status()
    
    # Stage 2 â†’ 3 ì „í™˜ ë¶„ì„
    print("\n3. Stage 2 â†’ 3 ì „í™˜")
    analysis = manager.handle_stage_transition(2, 3)
    
    manager.print_status()
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")