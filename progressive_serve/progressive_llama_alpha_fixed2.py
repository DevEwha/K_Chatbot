"""
ProgressiveLlamaModel with Alpha Gating (vLLM v0 Engine) - CUDA Graph í˜¸í™˜ ë²„ì „
progressive_llama_alpha_fixed2.py

ê¸°ì¡´ progressive_llama_alpha.pyì™€ì˜ ì°¨ì´:
- activate_layers() ë©”ì„œë“œë§Œ ìˆ˜ì •
- ë‚˜ë¨¸ì§€ëŠ” ë™ì¼

1. load_state_dict() â†’ .copy_()ë¡œ ë³€ê²½ (ë©”ëª¨ë¦¬ ì£¼ì†Œ ìœ ì§€)
2. fused_weightsë¥¼ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
3. CUDA Graph ì¬ìº¡ì²˜ ë¶ˆí•„ìš”


"""

from typing import Optional, List, Dict, Any
import torch
import torch.nn as nn
from vllm.config import VllmConfig
from vllm.model_executor.layers.vocab_parallel_embedding import VocabParallelEmbedding
from vllm.model_executor.layers.layernorm import RMSNorm
from vllm.model_executor.layers.sampler import SamplerOutput
# vLLM v0 imports
try:
    from vllm.attention import AttentionMetadata
except ImportError:
    try:
        from vllm.attention.backends.abstract import AttentionMetadata
    except ImportError:
        AttentionMetadata = Any

try:
    from vllm.sequence import IntermediateTensors
except ImportError:
    IntermediateTensors = Any

from safetensors.torch import load_file
import os

from alpha_gated_layer import AlphaGatedLayer


class ProgressiveLlamaModelAlpha(nn.Module):
    """
    Alpha Gatingì„ ì‚¬ìš©í•œ ProgressiveLlamaModel (vLLM v0)
    
    âœ… CUDA Graph í˜¸í™˜ ë²„ì „:
    - .copy_()ë¡œ weight ë¡œë“œ (ë©”ëª¨ë¦¬ ì£¼ì†Œ ìœ ì§€)
    - Alpha bufferëŠ” register_buffer + fill_() (ì£¼ì†Œ ê³ ì •)
    - ë ˆì´ì–´ êµì²´ ì—†ìŒ (ëª¨ë“  ë ˆì´ì–´ ì²˜ìŒë¶€í„° í• ë‹¹)
    """
    
    
    def __init__(
        self,
        vllm_config: VllmConfig,
        prefix: str = "",
        pruned_layer_indices: Optional[List[int]] = None,
    ):
        super().__init__()
        
        config = vllm_config.model_config.hf_config
        self.config = config
        self.vllm_config = vllm_config
        
        # Pruned layer ì¸ë±ìŠ¤ (ì´ˆê¸°ì— ë¹„í™œì„±í™”í•  ë ˆì´ì–´)
        self.initially_inactive = set(pruned_layer_indices or [])
        
        # Embedding
        self.embed_tokens = VocabParallelEmbedding(
            config.vocab_size,
            config.hidden_size,
        )
        
        # Decoder layers ì´ˆê¸°í™”
        self.layers = nn.ModuleList()
        self._init_layers(prefix)
        
        # Final norm
        self.norm = RMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps
        )
        
        # Adapter ìƒíƒœ
        self.current_adapter = None
    
    def _init_layers(self, prefix: str):
        """
        ëª¨ë“  ë ˆì´ì–´ ì´ˆê¸°í™” (AlphaGatedLayerë¡œ ê°ì‹¸ê¸°)
        
        í•µì‹¬:
        - ëª¨ë“  ë ˆì´ì–´ë¥¼ ìƒì„± (pruned layerë„!)
        - Pruned layerëŠ” weightë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        - AlphaGatedLayerë¡œ ê°ì‹¸ì„œ alpha=0 ì„¤ì •
        """
        # vLLM ë²„ì „ë³„ import ë¶„ê¸°
        try:
            from vllm.model_executor.models.llama import LlamaDecoderLayer
        except ImportError:
            try:
                from vllm.models.llama import LlamaDecoderLayer
            except ImportError:
                # v1 engine
                from vllm.v1.model_executor.models.llama import LlamaDecoderLayer
        
        num_layers = self.config.num_hidden_layers
        
        for layer_idx in range(num_layers):
            # ë ˆì´ì–´ ìƒì„± (í•­ìƒ!)
            try:
                # ìµœì‹  vLLM ë˜ëŠ” v1
                base_layer = LlamaDecoderLayer(
                    config=self.config,
                    cache_config=self.vllm_config.cache_config,
                    quant_config=self.vllm_config.quant_config,
                    prefix=f"{prefix}.layers.{layer_idx}",
                )
            except TypeError:
                # êµ¬ë²„ì „ fallback
                try:
                    base_layer = LlamaDecoderLayer(
                        layer_idx=layer_idx,
                        config=self.config,
                        prefix=f"{prefix}.layers.{layer_idx}",
                    )
                except TypeError:
                    # v0 ì—”ì§„ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤
                    base_layer = LlamaDecoderLayer(
                        vllm_config=self.vllm_config,
                        prefix=f"{prefix}.layers.{layer_idx}",
                    )
            
            # Alpha gating ì ìš©
            if layer_idx in self.initially_inactive:
                # Pruned layer: alpha = 0 (ë¹„í™œì„±)
                print(f"[Init] Layer {layer_idx:2d}: AlphaGatedLayer (alpha=0, INACTIVE)")
                
                # Weightë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
                self._initialize_weights_to_zero(base_layer)
                
                # AlphaGatedLayerë¡œ ê°ì‹¸ê¸°
                gated_layer = AlphaGatedLayer(
                    base_layer=base_layer,
                    initial_alpha=0.0,  # ë¹„í™œì„±
                )
                self.layers.append(gated_layer)
            else:
                # Normal layer: alpha = 1 (í™œì„±)
                print(f"[Init] Layer {layer_idx:2d}: AlphaGatedLayer (alpha=1, ACTIVE)")
                
                # AlphaGatedLayerë¡œ ê°ì‹¸ê¸°
                gated_layer = AlphaGatedLayer(
                    base_layer=base_layer,
                    initial_alpha=1.0,  # í™œì„±
                )
                self.layers.append(gated_layer)
    
    def _initialize_weights_to_zero(self, layer: nn.Module):
        """
        ë ˆì´ì–´ì˜ ëª¨ë“  weightë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        
        Note: alpha=0ì´ë¯€ë¡œ ì¶œë ¥ì— ì˜í–¥ ì—†ìŒ
              ë‚˜ì¤‘ì— ì‹¤ì œ weight ë¡œë“œ ì‹œ ë®ì–´ì”€
        """
        for param in layer.parameters():
            nn.init.zeros_(param)
    
    # ============================================================
    # vLLM Required Methods
    # ============================================================
    
    def embed_input_ids(self, input_ids: torch.Tensor) -> torch.Tensor:
        """í† í° ID â†’ ì„ë² ë”©"""
        return self.embed_tokens(input_ids)
    
    def forward(self, input_ids, positions, kv_caches, attn_metadata):
        hidden_states = self.embed_tokens(input_ids)
        
        residual = None
        
        # ëª¨ë“  ë ˆì´ì–´ í†µê³¼
        for i, layer in enumerate(self.layers):
            hidden_states, residual = layer(positions, hidden_states, residual)
        
        if residual is not None:
            hidden_states = hidden_states + residual
        
        hidden_states = self.norm(hidden_states)
        
        return hidden_states
    
    # ============================================================
    # Progressive Recovery Methods (Alpha Gating) - CUDA Graph í˜¸í™˜
    # ============================================================
    
    def activate_layers(
        self,
        layer_indices: List[int],
        checkpoint_path: str,
    ) -> None:
        """
        ë ˆì´ì–´ í™œì„±í™” (alpha: 0 â†’ 1) - âœ… CUDA Graph í˜¸í™˜
        
        í•µì‹¬ ë³€ê²½:
        1. load_state_dict() â†’ .copy_() ì‚¬ìš©
        2. ë©”ëª¨ë¦¬ ì£¼ì†Œ ìœ ì§€
        3. CUDA Graph ì¬ìº¡ì²˜ ë¶ˆí•„ìš”
        
        Args:
            layer_indices: í™œì„±í™”í•  ë ˆì´ì–´ ë²ˆí˜¸
            checkpoint_path: Weight íŒŒì¼ ê²½ë¡œ
        """
        print(f"\n{'='*60}")
        print(f"ACTIVATING LAYERS: {layer_indices}")
        print(f"{'='*60}")
        
        # Checkpoint ë¡œë“œ
        print(f"Loading checkpoint from: {checkpoint_path}")
        state_dict = load_file(checkpoint_path)
        
        device = next(self.parameters()).device
        
        for layer_idx in layer_indices:
            print(f"\nğŸ“‚ Activating layer {layer_idx}...")
            
            gated_layer = self.layers[layer_idx]
            
            # AlphaGatedLayer í™•ì¸
            if not hasattr(gated_layer, 'is_alpha_gated'):
                print(f"  âš ï¸  Layer {layer_idx} is not AlphaGatedLayer!")
                continue
            
            # ì´ë¯¸ í™œì„±í™”ëœ ë ˆì´ì–´
            if gated_layer.is_active():
                print(f"  â„¹ï¸  Layer {layer_idx} is already active")
                continue
            
            # 1. Weight ì¶”ì¶œ
            print(f"  ğŸ”¥ Loading weights...")
            layer_prefix = f"model.layers.{layer_idx}."
            layer_weights = {
                k.replace(layer_prefix, ""): v
                for k, v in state_dict.items()
                if k.startswith(layer_prefix)
            }
            
            if not layer_weights:
                print(f"  âš ï¸  No weights found for layer {layer_idx}")
                continue
            
            # 2. âœ… .copy_()ë¡œ in-place weight ë¡œë“œ (CUDA Graph í˜¸í™˜!)
            loaded_count = 0
            
            for name, param in gated_layer.layer.named_parameters():
                # 2.1. QKV fusion ì²˜ë¦¬
                if name == "self_attn.qkv_proj.weight":
                    if all(k in layer_weights for k in [
                        "self_attn.q_proj.weight",
                        "self_attn.k_proj.weight", 
                        "self_attn.v_proj.weight"
                    ]):
                        qkv_weight = torch.cat([
                            layer_weights["self_attn.q_proj.weight"],
                            layer_weights["self_attn.k_proj.weight"],
                            layer_weights["self_attn.v_proj.weight"]
                        ], dim=0)
                        
                        # âœ… í•µì‹¬: .copy_() ì‚¬ìš©! (ë©”ëª¨ë¦¬ ì£¼ì†Œ ìœ ì§€)
                        param.data.copy_(qkv_weight.to(device))
                        loaded_count += 1
                        print(f"  âœ… Loaded fused QKV")
                        continue
                
                # 2.2. Gate-Up fusion ì²˜ë¦¬
                if name == "mlp.gate_up_proj.weight":
                    if all(k in layer_weights for k in [
                        "mlp.gate_proj.weight",
                        "mlp.up_proj.weight"
                    ]):
                        gate_up_weight = torch.cat([
                            layer_weights["mlp.gate_proj.weight"],
                            layer_weights["mlp.up_proj.weight"]
                        ], dim=0)
                        
                        # âœ… í•µì‹¬: .copy_() ì‚¬ìš©! (ë©”ëª¨ë¦¬ ì£¼ì†Œ ìœ ì§€)
                        param.data.copy_(gate_up_weight.to(device))
                        loaded_count += 1
                        print(f"  âœ… Loaded fused Gate-Up")
                        continue
                
                # 2.3. ì¼ë°˜ weights ì²˜ë¦¬
                if name in layer_weights:
                    # âœ… í•µì‹¬: .copy_() ì‚¬ìš©! (ë©”ëª¨ë¦¬ ì£¼ì†Œ ìœ ì§€)
                    param.data.copy_(layer_weights[name].to(device))
                    loaded_count += 1
            
            print(f"  âœ… Loaded {loaded_count} weight tensors")
            
            # 3. Alpha í™œì„±í™” (0 â†’ 1)
            gated_layer.activate()
            
            # 4. initially_inactiveì—ì„œ ì œê±°
            self.initially_inactive.discard(layer_idx)
            
            print(f"  âœ… Layer {layer_idx} activated!")
        
        print(f"\n{'='*60}")
        print(f"LAYER ACTIVATION COMPLETE")
        print(f"Inactive layers: {self.count_inactive_layers()}")
        print(f"âœ… CUDA Graph ìœ ì§€ë¨ (ì¬ìº¡ì²˜ ë¶ˆí•„ìš”)")
        print(f"{'='*60}\n")
    
    # ============================================================
    # Status Methods
    # ============================================================
    
    def get_layer_status(self) -> Dict[int, Dict]:
        """ë ˆì´ì–´ ìƒíƒœ í™•ì¸"""
        status = {}
        for i, layer in enumerate(self.layers):
            if hasattr(layer, 'is_alpha_gated'):
                status[i] = {
                    "type": "AlphaGatedLayer",
                    "active": layer.is_active(),
                    "alpha": layer.get_alpha(),
                }
            else:
                status[i] = {
                    "type": "Unknown",
                    "active": True,
                    "alpha": 1.0,
                }
        return status
    
    def count_inactive_layers(self) -> int:
        """ë¹„í™œì„± ë ˆì´ì–´ ê°œìˆ˜"""
        count = 0
        for layer in self.layers:
            if hasattr(layer, 'is_alpha_gated') and not layer.is_active():
                count += 1
        return count
    
    def print_layer_status(self) -> None:
        """ë ˆì´ì–´ ìƒíƒœ ì¶œë ¥"""
        status = self.get_layer_status()
        
        print("\n" + "="*60)
        print("LAYER STATUS (Alpha Gating - CUDA Graph Compatible)")
        print("="*60)
        
        for start in range(0, len(status), 10):
            end = min(start + 10, len(status))
            print(f"\nLayers {start:2d}-{end-1:2d}:")
            
            for i in range(start, end):
                info = status[i]
                active = info['active']
                alpha = info['alpha']
                symbol = "â—‰" if active else "âŠ—"
                print(f"  L{i:2d}: {symbol} alpha={alpha:.1f} ({'ACTIVE' if active else 'INACTIVE'})")
        
        # Summary
        total = len(self.layers)
        inactive = self.count_inactive_layers()
        active = total - inactive
        progress = (active / total) * 100
        
        print(f"\n{'='*60}")
        print("SUMMARY")
        print(f"{'='*60}")
        print(f"Total Layers:         {total}")
        print(f"Active Layers:        {active}")
        print(f"Inactive Layers:      {inactive}")
        print(f"Activation Progress:  {progress:.1f}%")
        print(f"Current Adapter:      {self.current_adapter or 'None'}")
        print(f"âœ… CUDA Graph:        Compatible (no recapture needed)")
        print(f"{'='*60}\n")
    
    def verify_recovery(self) -> Dict:
        """ë³µêµ¬ ìƒíƒœ ê²€ì¦"""
        total = len(self.layers)
        inactive = self.count_inactive_layers()
        active = total - inactive
        
        inactive_indices = [
            i for i, layer in enumerate(self.layers)
            if hasattr(layer, 'is_alpha_gated') and not layer.is_active()
        ]
        
        progress = (active / total) * 100
        
        return {
            "total_layers": total,
            "active_layers": active,
            "inactive_layers": inactive,
            "inactive_layer_indices": inactive_indices,
            "activation_progress": f"{progress:.1f}%",
            "cuda_graph_compatible": True,
        }
    
    def get_adapter_info(self) -> Dict:
        """Adapter ì •ë³´"""
        return {
            "current_adapter": self.current_adapter,
            "has_adapter": self.current_adapter is not None,
        }


# ============================================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================================

if __name__ == "__main__":
    print("""
Progressive LLaMA Alpha Gating - CUDA Graph í˜¸í™˜ ë²„ì „
====================================================

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. âœ… .copy_()ë¡œ weight ë¡œë“œ (ë©”ëª¨ë¦¬ ì£¼ì†Œ ìœ ì§€)
2. âœ… Alpha bufferëŠ” register_buffer + fill_() (ì£¼ì†Œ ê³ ì •)
3. âœ… CUDA Graph ì¬ìº¡ì²˜ ë¶ˆí•„ìš”

í…ŒìŠ¤íŠ¸:
    import torch
    from vllm.config import VllmConfig
    from progressive_llama_alpha_fixed import ProgressiveLlamaModelAlpha
    
    # ì´ˆê¸°í™”
    model = ProgressiveLlamaModelAlpha(...)
    model.cuda()
    model.eval()
    
    # CUDA Graph ìº¡ì²˜
    g = torch.cuda.CUDAGraph()
    with torch.cuda.graph(g):
        out1 = model.forward(...)
    
    # Stage ì „í™˜
    model.activate_layers([21, 22, 23, 24], "stage2.safetensors")
    
    # Graph ì¬ì‹¤í–‰ (ì¬ìº¡ì²˜ ì—†ì´!)
    g.replay()  # âœ… ì„±ê³µ!
""")