"""
Alpha Gating Layer for ProgressiveServe (vLLM v0 Compatible)
alpha_gated_layer.py

CUDA Graph í˜¸í™˜ ë™ì  ë ˆì´ì–´ í™œì„±í™”
"""

import torch
import torch.nn as nn
from typing import Optional, Tuple
import torch.cuda.nvtx as nvtx

class AlphaGatedLayer(nn.Module):
    """
    Alpha Gatingì„ ì‚¬ìš©í•œ ë™ì  ë ˆì´ì–´ í™œì„±í™” (vLLM v0)
    
    í•µì‹¬ ì•„ì´ë””ì–´:
        y = x + alpha * F(x)
        
    - alpha = 0: ë ˆì´ì–´ ë¹„í™œì„± (Pass through)
    - alpha = 1: ë ˆì´ì–´ í™œì„± (Normal operation)
    
    ì¥ì :
    
    1. CUDA Graph í˜¸í™˜: ì»¤ë„ ê°œìˆ˜ í•­ìƒ ë™ì¼
    2. ë™ì  í™œì„±í™”: alphaë§Œ ë³€ê²½í•˜ë©´ ë¨
    3. Weight 0 ì´ˆê¸°í™”: ì¶”ë¡ ì— ì˜í–¥ ì—†ìŒ
    
    vLLM v0 í˜¸í™˜:
    - forward ì‹œê·¸ë‹ˆì²˜: (positions, hidden_states, residual)
    - kv_cacheì™€ attn_metadataëŠ” vLLM ë‚´ë¶€ì—ì„œ ì²˜ë¦¬
    """
    
    def __init__(
        self,
        base_layer: nn.Module,
        initial_alpha: float = 0.0,
    ):
        """
        Args:
            base_layer: ì‹¤ì œ LlamaDecoderLayer
            initial_alpha: ì´ˆê¸° alpha ê°’ (0.0 = ë¹„í™œì„±)
        """
        super().__init__()
        
        # ì‹¤ì œ ë ˆì´ì–´ (í•­ìƒ ì¡´ì¬)
        self.layer = base_layer
        
        # Alpha gate (learnable parameterëŠ” ì•„ë‹˜)
        self.register_buffer('alpha', torch.tensor(initial_alpha))
        
        # í™œì„±í™” ìƒíƒœ í”Œë˜ê·¸
        self._is_active = initial_alpha > 0.5
    
    def forward(self, positions, hidden_states, residual):

        
        # base layerê°€ residual ê´€ë¦¬ (ìœ„ì„)
        nvtx.range_push("BaseLayer")
        delta, updated_residual = self.layer(positions, hidden_states, residual)
        nvtx.range_pop()
        
        # alpha gating ì ìš©
        nvtx.range_push("AlphaMultiply")
        gated_delta = self.alpha * delta
        nvtx.range_pop()
    

        # vLLM í‘œì¤€: (delta, residual) ë°˜í™˜ (í•©ì¹˜ì§€ ì•ŠìŒ!)
        return gated_delta, updated_residual
    
    def activate(self):
        """ë ˆì´ì–´ í™œì„±í™” (alpha = 1)"""
        
        self.alpha.fill_(1.0)
        self._is_active = True
        print(f"âœ…  ğŸ’›Layer activated (alpha = 1.0)")
    
    def deactivate(self):
        """ë ˆì´ì–´ ë¹„í™œì„±í™” (alpha = 0)"""
        self.alpha.fill_(0.0)
        self._is_active = False
        print(f"âŠ— Layer deactivated (alpha = 0.0)")
    
    def is_active(self) -> bool:
        """í™œì„±í™” ì—¬ë¶€ í™•ì¸"""
        return self._is_active
    
    def get_alpha(self) -> float:
        """í˜„ì¬ alpha ê°’"""
        return self.alpha.item()
    
    @property
    def is_alpha_gated(self) -> bool:
        """AlphaGatedLayer ì‹ë³„ìš©"""
        return True