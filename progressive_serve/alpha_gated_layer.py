"""
Alpha Gating Layer for ProgressiveServe (vLLM v0 Compatible)
progressive_serve/alpha_gated_layer.py

CUDA Graph í˜¸í™˜ ë™ì  ë ˆì´ì–´ í™œì„±í™” + Split KV Cache ì§€ì›
"""

import torch
import torch.nn as nn
from typing import Optional, Tuple

try:
    import torch.cuda.nvtx as nvtx
    HAS_NVTX = True
except ImportError:
    HAS_NVTX = False
    class DummyNVTX:
        @staticmethod
        def range_push(msg): pass
        @staticmethod
        def range_pop(): pass
    nvtx = DummyNVTX()


class AlphaGatedLayer(nn.Module):
    """
    Alpha Gatingì„ ì‚¬ìš©í•œ ë™ì  ë ˆì´ì–´ í™œì„±í™” (vLLM v0)
    
    í•µì‹¬ ì•„ì´ë””ì–´:
        y = x + alpha * F(x)
        
    - alpha = 0: ë ˆì´ì–´ ë¹„í™œì„± (Pass through)
    - alpha = 1: ë ˆì´ì–´ í™œì„± (Normal operation)
    
    ìž¥ì :
    1. CUDA Graph í˜¸í™˜: ì»¤ë„ ê°œìˆ˜ í•­ìƒ ë™ì¼
    2. ë™ì  í™œì„±í™”: alphaë§Œ ë³€ê²½í•˜ë©´ ë¨
    3. Weight 0 ì´ˆê¸°í™”: ì¶”ë¡ ì— ì˜í–¥ ì—†ìŒ
    
    vLLM v0 í˜¸í™˜:
    - forward ì‹œê·¸ë‹ˆì²˜: (positions, hidden_states, residual)
    - kv_cacheì™€ attn_metadataëŠ” vLLM ë‚´ë¶€ì—ì„œ ì²˜ë¦¬
    
    Split KV Cache ì§€ì›:
    - split_cache_managerë¥¼ í†µí•œ Base/Delta ë¶„ë¦¬ ìºì‹±
    """
    
    def __init__(
        self,
        base_layer: nn.Module,
        initial_alpha: float = 0.0,
    ):
        """
        Args:
            base_layer: ì‹¤ì œ LlamaDecoderLayer
            initial_alpha: ì´ˆê¸° alpha ê°’ (0.0 = ë¹„í™œì„±)
        """
        super().__init__()
        
        # ì‹¤ì œ ë ˆì´ì–´ (í•­ìƒ ì¡´ìž¬)
        self.layer = base_layer
        
        # Alpha gate (learnable parameterëŠ” ì•„ë‹˜)
        self.register_buffer('alpha', torch.tensor(initial_alpha))
        
        # í™œì„±í™” ìƒíƒœ í”Œëž˜ê·¸
        self._is_active = initial_alpha > 0.5
        
        # ë ˆì´ì–´ ì¸ë±ìŠ¤ (ë‚˜ì¤‘ì— ì„¤ì •ë¨)
        self._layer_idx: Optional[int] = None
        
        # Split KV Cache Manager (ì™¸ë¶€ì—ì„œ ì„¤ì •)
        self._split_cache_manager = None
    
    def set_layer_idx(self, idx: int):
        """ë ˆì´ì–´ ì¸ë±ìŠ¤ ì„¤ì •"""
        self._layer_idx = idx
    
    def set_split_cache_manager(self, manager):
        """Split Cache Manager ì„¤ì •"""
        self._split_cache_manager = manager
    
    @property
    def layer_idx(self) -> Optional[int]:
        """ë ˆì´ì–´ ì¸ë±ìŠ¤"""
        return self._layer_idx
    
    def forward(
        self, 
        positions: torch.Tensor, 
        hidden_states: torch.Tensor, 
        residual: Optional[torch.Tensor]
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Forward pass with Alpha Gating
        
        Args:
            positions: Position IDs
            hidden_states: Input hidden states
            residual: Residual tensor (from previous layer)
            
        Returns:
            (gated_delta, updated_residual)
        """
        # base layerê°€ residual ê´€ë¦¬ (ìœ„ìž„)
        nvtx.range_push("BaseLayer")
        delta, updated_residual = self.layer(positions, hidden_states, residual)
        nvtx.range_pop()
        
        # alpha gating ì ìš©
        nvtx.range_push("AlphaMultiply")
        gated_delta = self.alpha * delta
        nvtx.range_pop()
        
        # vLLM í‘œì¤€: (delta, residual) ë°˜í™˜ (í•©ì¹˜ì§€ ì•ŠìŒ!)
        return gated_delta, updated_residual
    
    def forward_with_split_cache(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        residual: Optional[torch.Tensor],
        use_cache: bool = True,
        recompute_delta_only: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Split KV Cacheë¥¼ í™œìš©í•œ Forward pass
        
        Args:
            positions: Position IDs
            hidden_states: Input hidden states
            residual: Residual tensor
            use_cache: Cache ì‚¬ìš© ì—¬ë¶€
            recompute_delta_only: Deltaë§Œ ìž¬ê³„ì‚° (Base ìž¬ì‚¬ìš©)
            
        Returns:
            (gated_delta, updated_residual)
            
        Note:
            ì´ ë©”ì„œë“œëŠ” Split KV Cacheê°€ Attention ë ˆë²¨ì—ì„œ í†µí•©ë  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
            í˜„ìž¬ëŠ” ê¸°ë³¸ forwardì™€ ë™ì¼í•˜ê²Œ ë™ìž‘í•©ë‹ˆë‹¤.
        """
        # í˜„ìž¬ëŠ” ê¸°ë³¸ forwardë¡œ ìœ„ìž„
        # ì¶”í›„ Attention ë ˆë²¨ í†µí•© ì‹œ í™•ìž¥
        return self.forward(positions, hidden_states, residual)
    
    def activate(self):
        """ë ˆì´ì–´ í™œì„±í™” (alpha = 1)"""
        self.alpha.fill_(1.0)
        self._is_active = True
        layer_str = f"Layer {self._layer_idx}" if self._layer_idx is not None else "Layer"
        print(f"âœ… ðŸ’› {layer_str} activated (alpha = 1.0)")
    
    def deactivate(self):
        """ë ˆì´ì–´ ë¹„í™œì„±í™” (alpha = 0)"""
        self.alpha.fill_(0.0)
        self._is_active = False
        layer_str = f"Layer {self._layer_idx}" if self._layer_idx is not None else "Layer"
        print(f"âŠ— {layer_str} deactivated (alpha = 0.0)")
    
    def is_active(self) -> bool:
        """í™œì„±í™” ì—¬ë¶€ í™•ì¸"""
        return self._is_active
    
    def get_alpha(self) -> float:
        """í˜„ìž¬ alpha ê°’"""
        return self.alpha.item()
    
    def set_alpha(self, value: float):
        """alpha ê°’ ì§ì ‘ ì„¤ì •"""
        self.alpha.fill_(value)
        self._is_active = value > 0.5
    
    @property
    def is_alpha_gated(self) -> bool:
        """AlphaGatedLayer ì‹ë³„ìš©"""
        return True
    
    def __repr__(self) -> str:
        layer_str = f"layer_idx={self._layer_idx}, " if self._layer_idx is not None else ""
        return f"AlphaGatedLayer({layer_str}alpha={self.get_alpha():.2f}, active={self._is_active})"