#!/usr/bin/env python3
"""
ì‹¤í–‰: python chatbot.py
ëŒ€í™”í˜• Progressive Stage í…ŒìŠ¤íŠ¸ (Interactive Version)
íŒŒì¼ëª…: chatbot.py

ìˆ˜ì • ì‚¬í•­:
1. ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ì¶”ê°€ (vLLM ë¡œë”© í•„ìˆ˜ ë‹¨ê³„)
2. Python Path ì„¤ì • ì¶”ê°€ (ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì°¸ì¡°ìš©)
3. Prefix Caching í™œì„±í™”ë¡œ ëŒ€í™” ë§¥ë½ ìœ ì§€
4. Stage ì „í™˜ ì‹œ KV Cache ì´ˆê¸°í™”
5. ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
6. ì‚¬ìš©ì ëª…ë ¹ìœ¼ë¡œ Stage ì „í™˜ ì œì–´
"""

import sys
import os
import time
import torch

# [í•„ìˆ˜] Python path ì„¤ì • - ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œë¥¼ ì¸ì‹í•˜ê²Œ í•©ë‹ˆë‹¤.
sys.path.insert(0, "/workspace/vllm_test")
sys.path.insert(0, "/acpl-ssd30/7b_results/pruning/A")
sys.path.insert(0, "/home/devewha/Juwon/vllm_test")

# vLLM import
try:
    from vllm import LLM, SamplingParams
    from vllm.model_executor.models.registry import ModelRegistry
    from progressive_llama_for_causal_lm_alpha_v0 import ProgressiveLlamaForCausalLMAlpha
    print("âœ… vLLM and Custom Model imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import required modules: {e}")
    sys.exit(1)

# [í•„ìˆ˜] ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
ModelRegistry.register_model(
    "ProgressiveLlamaForCausalLM",
    ProgressiveLlamaForCausalLMAlpha
)


class ProgressiveChatbot:
    """ëŒ€í™”í˜• Progressive Stage ì±—ë´‡"""
    
    def __init__(self, model_path, stage2_path, stage3_path):
        self.model_path = model_path
        self.stage2_path = stage2_path
        self.stage3_path = stage3_path
        self.current_stage = 1
        self.conversation_history = ""
        self.turn_count = 0
        
        # í†µê³„ ì •ë³´
        self.stage_stats = {
            1: {"inference_times": [], "token_counts": []},
            2: {"inference_times": [], "token_counts": []},
            3: {"inference_times": [], "token_counts": []}
        }
        
        self.llm = None
        self.model = None
        self.sampling_params = None
        
    def initialize(self):
        """vLLM ì—”ì§„ ì´ˆê¸°í™”"""
        print("\n" + "="*80)
        print("ğŸš€ Progressive LLM Chatbot - Initialization")
        print("="*80 + "\n")
        
        print("â³ Initializing vLLM with Prefix Caching enabled...")
        start_init = time.time()
        
        try:
            self.llm = LLM(
                model=self.model_path,
                trust_remote_code=True,
                gpu_memory_utilization=0.9,
                enforce_eager=False,
                enable_prefix_caching=True  # Prefix caching í™œì„±í™”!
            )
        except Exception as e:
            print(f"âŒ Failed to initialize vLLM: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
        
        init_time = time.time() - start_init
        print(f"âœ… Initialization complete: {init_time:.2f}s\n")
        
        # ëª¨ë¸ ê°ì²´ ì§ì ‘ ì ‘ê·¼
        try:
            self.model = self.llm.llm_engine.model_executor.driver_worker.model_runner.model
            print(f"âœ… Model accessed: {type(self.model).__name__}")
        except Exception as e:
            print(f"âŒ Failed to access model: {e}")
            sys.exit(1)
        
        # Sampling íŒŒë¼ë¯¸í„° ì„¤ì •
        self.sampling_params = SamplingParams(
            max_tokens=100,
            temperature=0.7,
            top_p=0.95,
        )
        
        print(f"âœ… Sampling parameters configured")
        print(f"   - max_tokens: 100")
        print(f"   - temperature: 0.7")
        print(f"   - top_p: 0.95")
        print(f"\nğŸ¯ Currently in Stage {self.current_stage}\n")
        
    def advance_stage(self, target_stage):
        """Stage ì „í™˜ (KV Cache ì´ˆê¸°í™” í¬í•¨)"""
        if target_stage == self.current_stage:
            print(f"âš ï¸  Already in Stage {target_stage}")
            return False
        
        if target_stage < self.current_stage:
            print(f"âš ï¸  Cannot downgrade from Stage {self.current_stage} to Stage {target_stage}")
            return False
        
        if target_stage > 3:
            print(f"âš ï¸  Invalid stage: {target_stage}. Valid stages: 1, 2, 3")
            return False
        
        print(f"\n{'='*80}")
        print(f"ğŸ”„ Stage Transition: {self.current_stage} â†’ {target_stage}")
        print(f"{'='*80}\n")
        
        # 1. KV Cache ì´ˆê¸°í™”
        print("ğŸ§¹ Clearing KV Cache...")
        try:
            self.llm.sleep(level=1)
            print("âœ… KV Cache cleared")
        except Exception as e:
            print(f"âš ï¸  Cache clear warning: {e}")
        
        # 2. Stage ì „í™˜
        start_transition = time.time()
        try:
            if target_stage == 2:
                print(f"ğŸ“¦ Loading Stage 2 layers from: {self.stage2_path}")
                self.model.advance_to_stage2(layer_b_checkpoint=self.stage2_path)
            elif target_stage == 3:
                if self.current_stage == 1:
                    print("âš ï¸  Must advance to Stage 2 first")
                    return False
                print(f"ğŸ“¦ Loading Stage 3 layers from: {self.stage3_path}")
                self.model.advance_to_stage3(layer_c_checkpoint=self.stage3_path)
            
            transition_time = time.time() - start_transition
            print(f"âœ… Stage {target_stage} transition complete: {transition_time:.2f}s")
        except Exception as e:
            print(f"âŒ Stage transition failed: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        # 3. ì—”ì§„ ì¬í™œì„±í™”
        try:
            self.llm.wake_up()
            print("âœ… Engine reactivated")
        except Exception as e:
            print(f"âš ï¸  Wake up warning: {e}")
        
        self.current_stage = target_stage
        
        # 4. ìƒíƒœ í™•ì¸
        try:
            self.model.print_status()
        except:
            try:
                info = self.model.get_stage_info()
                print(f"   Current stage: {info.get('stage')}")
            except:
                pass
        
        print(f"\nğŸ¯ Now in Stage {self.current_stage}\n")
        return True
    
    def generate_response(self, user_input):
        """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        self.turn_count += 1
        
        # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if self.conversation_history:
            full_prompt = self.conversation_history + f"\nUser: {user_input}\nAssistant:"
        else:
            full_prompt = f"User: {user_input}\nAssistant:"
        
        # ì¶”ë¡  ì‹¤í–‰
        print(f"\nğŸ’­ Thinking... (Stage {self.current_stage})")
        start_time = time.time()
        
        try:
            outputs = self.llm.generate([full_prompt], self.sampling_params)
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            return None
        
        elapsed_time = time.time() - start_time
        
        # ì‘ë‹µ ì¶”ì¶œ
        response = outputs[0].outputs[0].text.strip()
        token_count = len(outputs[0].outputs[0].token_ids)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stage_stats[self.current_stage]["inference_times"].append(elapsed_time)
        self.stage_stats[self.current_stage]["token_counts"].append(token_count)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.conversation_history = full_prompt + " " + response
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*80}")
        print(f"ğŸ¤– Assistant (Stage {self.current_stage} - Turn {self.turn_count})")
        print(f"{'='*80}")
        print(f"{response}")
        print(f"{'-'*80}")
        print(f"â±ï¸  Inference time: {elapsed_time:.4f}s")
        print(f"ğŸ”¢ Generated tokens: {token_count}")
        print(f"ğŸ“ Context length: {len(full_prompt.split())} words")
        print(f"{'='*80}\n")
        
        return response
    
    def print_statistics(self):
        """ëŒ€í™” í†µê³„ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print("ğŸ“Š Session Statistics")
        print(f"{'='*80}")
        print(f"Total turns: {self.turn_count}")
        print(f"Final stage: {self.current_stage}")
        print(f"Context length: {len(self.conversation_history.split())} words\n")
        
        for stage in [1, 2, 3]:
            times = self.stage_stats[stage]["inference_times"]
            tokens = self.stage_stats[stage]["token_counts"]
            
            if times:
                avg_time = sum(times) / len(times)
                avg_tokens = sum(tokens) / len(tokens)
                print(f"Stage {stage}:")
                print(f"  - Turns: {len(times)}")
                print(f"  - Avg inference time: {avg_time:.4f}s")
                print(f"  - Avg tokens generated: {avg_tokens:.1f}")
                print(f"  - Total time: {sum(times):.4f}s")
        
        print(f"{'='*80}\n")
    
    def print_help(self):
        """ë„ì›€ë§ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print("ğŸ“– Available Commands")
        print(f"{'='*80}")
        print("/stage2    - Advance to Stage 2 (load B layers)")
        print("/stage3    - Advance to Stage 3 (load C layers)")
        print("/stats     - Show session statistics")
        print("/clear     - Clear conversation history")
        print("/help      - Show this help message")
        print("/exit      - Exit the chatbot")
        print("\nOr just type your message to chat!")
        print(f"{'='*80}\n")
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = ""
        print("âœ… Conversation history cleared\n")
    
    def run(self):
        """ëŒ€í™”í˜• ë£¨í”„ ì‹¤í–‰"""
        self.initialize()
        self.print_help()
        
        print("ğŸ’¬ Chat started! Type your message or use commands.\n")
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
                user_input = input(f"[Stage {self.current_stage}] You: ").strip()
                
                if not user_input:
                    continue
                
                # ëª…ë ¹ì–´ ì²˜ë¦¬
                if user_input.startswith("/"):
                    command = user_input.lower()
                    
                    if command == "/exit" or command == "/quit":
                        print("\nğŸ‘‹ Goodbye!")
                        self.print_statistics()
                        break
                    
                    elif command == "/stage2":
                        self.advance_stage(2)
                    
                    elif command == "/stage3":
                        self.advance_stage(3)
                    
                    elif command == "/stats":
                        self.print_statistics()
                    
                    elif command == "/clear":
                        self.clear_history()
                    
                    elif command == "/help":
                        self.print_help()
                    
                    else:
                        print(f"âš ï¸  Unknown command: {user_input}")
                        print("Type /help to see available commands\n")
                
                # ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
                else:
                    self.generate_response(user_input)
            
            except KeyboardInterrupt:
                print("\n\nâš ï¸  Interrupted by user")
                self.print_statistics()
                break
            
            except Exception as e:
                print(f"âŒ Error: {e}")
                import traceback
                traceback.print_exc()


def main():
    # ì„¤ì • ê²½ë¡œ
    model_path = "/acpl-ssd30/7b_results/pruning/A"
    stage2_path = "/acpl-ssd30/7b_results/pruning/checkpoints/stage2_layers_B.safetensors"
    stage3_path = "/acpl-ssd30/7b_results/pruning/checkpoints/stage3_layers_C.safetensors"
    
    # ì±—ë´‡ ìƒì„± ë° ì‹¤í–‰
    chatbot = ProgressiveChatbot(
        model_path=model_path,
        stage2_path=stage2_path,
        stage3_path=stage3_path
    )
    
    chatbot.run()


if __name__ == "__main__":
    main()