#!/usr/bin/env python3
"""
Progressive Stage Chatbot with KV Cache Reset
íŒŒì¼ëª…: chatbot.py

ì‚¬ìš©í•˜ëŠ” íŒŒì¼ë“¤:
1. progressive_llama_for_causal_lm_alpha_v0.py - ë©”ì¸ ëª¨ë¸ í´ë˜ìŠ¤
2. progressive_llama_alpha_fixed.py (ë˜ëŠ” progressive_llama_alpha_fixed2.py) - CUDA Graph í˜¸í™˜ ëª¨ë¸
3. alpha_gated_layer.py - Alpha Gating ë ˆì´ì–´

ê¸°ëŠ¥:
- Stage 1/2/3 ì „í™˜ ì§€ì›
- Stage ì „í™˜ ì‹œ KV Cache ì´ˆê¸°í™” í›„ ë§¥ë½ ì¬ê³„ì‚°
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€
- Prefix caching í™œìš©

ëª…ë ¹ì–´:
- quit: ì¢…ë£Œ
- reset: KV cache ì´ˆê¸°í™” ë° ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¦¬ì…‹
- stage2: Stage 2ë¡œ ì „í™˜ (KV cache ì´ˆê¸°í™” í›„ ë§¥ë½ ì¬ê³„ì‚°)
- stage3: Stage 3ë¡œ ì „í™˜ (KV cache ì´ˆê¸°í™” í›„ ë§¥ë½ ì¬ê³„ì‚°)
- status: í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì¶œë ¥
"""

import sys
import os
import time
import torch

# Python path ì„¤ì • - ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ ì¸ì‹
sys.path.insert(0, "/workspace/vllm_test")
sys.path.insert(0, "/acpl-ssd20/1218/A")
sys.path.insert(0, "/home/devewha/Juwon/vllm_test")

# vLLM import
try:
    from vllm import LLM, SamplingParams
    from vllm.model_executor.models.registry import ModelRegistry
    from progressive_serve.progressive_llama_for_causal_lm_alpha_v0 import ProgressiveLlamaForCausalLMAlpha
    print("âœ… vLLM and Custom Model imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import required modules: {e}")
    print("í•„ìš”í•œ íŒŒì¼ë“¤:")
    print("  - progressive_serve/progressive_llama_for_causal_lm_alpha_v0.py")
    print("  - progressive_serve/progressive_llama_alpha_fixed.py")
    print("  - progressive_serve/alpha_gated_layer.py")
    sys.exit(1)

# ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
ModelRegistry.register_model(
    "ProgressiveLlamaForCausalLM",
    ProgressiveLlamaForCausalLMAlpha
)


class ProgressiveChatbot:
    """
    Progressive Stageë¥¼ ì§€ì›í•˜ëŠ” vLLM ì±—ë´‡
    
    Stage ì „í™˜ ì‹œ:
    1. KV Cache ì´ˆê¸°í™” (sleep/wake_up)
    2. ëŒ€í™” ë§¥ë½ ì¬ê³„ì‚°
    """
    
    def __init__(
        self,
        model_path: str,
        stage2_checkpoint: str,
        stage3_checkpoint: str,
        gpu_memory_utilization: float = 0.9,
        enable_prefix_caching: bool = True,
        enforce_eager: bool = False,
    ):
        """
        Args:
            model_path: Stage 1 ëª¨ë¸ ê²½ë¡œ
            stage2_checkpoint: Stage 2 weights ê²½ë¡œ
            stage3_checkpoint: Stage 3 weights ê²½ë¡œ
            gpu_memory_utilization: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            enable_prefix_caching: Prefix caching í™œì„±í™”
            enforce_eager: CUDA Graph ë¹„í™œì„±í™” (Trueë©´ ë™ì  ë ˆì´ì–´ ë³€ê²½ ì•ˆì •)
        """
        self.model_path = model_path
        self.stage2_checkpoint = stage2_checkpoint
        self.stage3_checkpoint = stage3_checkpoint
        
        # vLLM ì—”ì§„ ì´ˆê¸°í™”
        print("\n" + "="*60)
        print("Initializing Progressive Chatbot...")
        print("="*60 + "\n")
        
        start_init = time.time()
        
        self.llm = LLM(
            model=model_path,
            trust_remote_code=True,
            gpu_memory_utilization=gpu_memory_utilization,
            enable_prefix_caching=enable_prefix_caching,
            enforce_eager=enforce_eager,
        )
        
        init_time = time.time() - start_init
        print(f"âœ… vLLM Initialization complete: {init_time:.2f}s\n")
        
        # ëª¨ë¸ ê°ì²´ ì ‘ê·¼
        try:
            self.model = self.llm.llm_engine.model_executor.driver_worker.model_runner.model
            print(f"âœ… Model accessed: {type(self.model).__name__}")
        except Exception as e:
            print(f"âŒ Failed to access model: {e}")
            raise
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
        self.sampling_params = SamplingParams(
            temperature=0.7,
            max_tokens=200,
        )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.history_prompt = ""
        
        # í˜„ì¬ Stage
        self.current_stage = 1
        
        print(f"âœ… Chatbot ready at Stage {self.current_stage}")
        print("-" * 60)
    
    def reset_kv_cache(self) -> float:
        """
        KV Cache ì´ˆê¸°í™”
        
        Returns:
            ì´ˆê¸°í™”ì— ê±¸ë¦° ì‹œê°„ (ì´ˆ)
        """
        print("ğŸ”„ Resetting KV cache...")
        start = time.time()
        
        try:
            # vLLM sleep/wake_upìœ¼ë¡œ KV cache ì´ˆê¸°í™”
            self.llm.sleep(level=1)  # KV cache ì‚­ì œ
            self.llm.wake_up()       # ì—”ì§„ ì¬ì‹œì‘
        except AttributeError:
            # sleep/wake_upì´ ì—†ëŠ” ë²„ì „ì˜ ê²½ìš°
            print("âš ï¸  sleep/wake_up not available, using alternative method...")
            # llm_engineì˜ cache ê´€ë ¨ ë©”ì„œë“œ í˜¸ì¶œ ì‹œë„
            try:
                if hasattr(self.llm.llm_engine, 'reset_prefix_cache'):
                    self.llm.llm_engine.reset_prefix_cache()
                elif hasattr(self.llm.llm_engine, 'scheduler'):
                    # Schedulerì˜ free_seq ë“±ì„ í†µí•œ ìš°íšŒ
                    pass
            except Exception as e:
                print(f"âš ï¸  Alternative KV reset failed: {e}")
        
        elapsed = time.time() - start
        print(f"âœ… KV cache reset complete ({elapsed:.2f}s)")
        return elapsed
    
    def recompute_context(self) -> float:
        """
        í˜„ì¬ ëŒ€í™” ë§¥ë½ì„ ìƒˆ Stageë¡œ ì¬ê³„ì‚°
        
        Stage ì „í™˜ í›„ ê¸°ì¡´ íˆìŠ¤í† ë¦¬ë¡œ ìƒˆë¡œìš´ KV cache ìƒì„±
        
        Returns:
            ì¬ê³„ì‚°ì— ê±¸ë¦° ì‹œê°„ (ì´ˆ)
        """
        if not self.history_prompt:
            print("â„¹ï¸  No history to recompute")
            return 0.0
        
        print("ğŸ”„ Recomputing context with new stage...")
        start = time.time()
        
        # ë¹ˆ ìƒì„± ìš”ì²­ìœ¼ë¡œ KV cacheë§Œ ìƒì„±
        # max_tokens=1ë¡œ ìµœì†Œí•œì˜ ìƒì„±ë§Œ ìˆ˜í–‰
        recompute_params = SamplingParams(
            temperature=0.0,
            max_tokens=1,  # ìµœì†Œ í† í°ë§Œ ìƒì„±
        )
        
        try:
            # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ë¡œ forward pass ìˆ˜í–‰ (KV cache ì¬êµ¬ì¶•)
            _ = self.llm.generate([self.history_prompt], recompute_params)
        except Exception as e:
            print(f"âš ï¸  Context recompute warning: {e}")
        
        elapsed = time.time() - start
        print(f"âœ… Context recomputed ({elapsed:.2f}s)")
        return elapsed
    
    def advance_to_stage2(self) -> bool:
        """
        Stage 2ë¡œ ì „í™˜
        
        1. ìƒˆ ë ˆì´ì–´ weights ë¡œë“œ ë° í™œì„±í™”
        2. KV Cache ì´ˆê¸°í™”
        3. ëŒ€í™” ë§¥ë½ ì¬ê³„ì‚°
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if self.current_stage >= 2:
            print(f"âš ï¸  Already at Stage {self.current_stage}")
            return False
        
        print("\n" + "="*60)
        print("TRANSITIONING TO STAGE 2")
        print("="*60 + "\n")
        
        start = time.time()
        
        try:
            # 1. Stage 2 ë ˆì´ì–´ í™œì„±í™”
            print("[Step 1/3] Activating Stage 2 layers...")
            self.model.advance_to_stage2(layer_b_checkpoint=self.stage2_checkpoint)
            
            # 2. KV Cache ì´ˆê¸°í™”
            print("\n[Step 2/3] Resetting KV cache...")
            self.reset_kv_cache()
            
            # 3. ëŒ€í™” ë§¥ë½ ì¬ê³„ì‚°
            print("\n[Step 3/3] Recomputing conversation context...")
            self.recompute_context()
            
            self.current_stage = 2
            
            total_time = time.time() - start
            print(f"\n{'='*60}")
            print(f"âœ… NOW AT STAGE 2 (Total: {total_time:.2f}s)")
            print(f"{'='*60}\n")
            
            return True
            
        except Exception as e:
            print(f"âŒ Stage 2 transition failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def advance_to_stage3(self) -> bool:
        """
        Stage 3ë¡œ ì „í™˜
        
        1. ìƒˆ ë ˆì´ì–´ weights ë¡œë“œ ë° í™œì„±í™”
        2. KV Cache ì´ˆê¸°í™”
        3. ëŒ€í™” ë§¥ë½ ì¬ê³„ì‚°
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if self.current_stage >= 3:
            print(f"âš ï¸  Already at Stage {self.current_stage}")
            return False
        
        if self.current_stage < 2:
            print("âš ï¸  Must be at Stage 2 first. Advancing to Stage 2...")
            if not self.advance_to_stage2():
                return False
        
        print("\n" + "="*60)
        print("TRANSITIONING TO STAGE 3")
        print("="*60 + "\n")
        
        start = time.time()
        
        try:
            # 1. Stage 3 ë ˆì´ì–´ í™œì„±í™”
            print("[Step 1/3] Activating Stage 3 layers...")
            self.model.advance_to_stage3(layer_c_checkpoint=self.stage3_checkpoint)
            
            # 2. KV Cache ì´ˆê¸°í™”
            print("\n[Step 2/3] Resetting KV cache...")
            self.reset_kv_cache()
            
            # 3. ëŒ€í™” ë§¥ë½ ì¬ê³„ì‚°
            print("\n[Step 3/3] Recomputing conversation context...")
            self.recompute_context()
            
            self.current_stage = 3
            
            total_time = time.time() - start
            print(f"\n{'='*60}")
            print(f"âœ… NOW AT STAGE 3 - FULL MODEL (Total: {total_time:.2f}s)")
            print(f"{'='*60}\n")
            
            return True
            
        except Exception as e:
            print(f"âŒ Stage 3 transition failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def print_status(self):
        """í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì¶œë ¥"""
        print("\n" + "="*60)
        print(f"CHATBOT STATUS")
        print("="*60)
        print(f"Current Stage: {self.current_stage}")
        print(f"History Length: {len(self.history_prompt)} chars")
        
        try:
            self.model.print_status()
        except:
            try:
                info = self.model.get_stage_info()
                print(f"Active Layers: {info.get('active_layers', 'N/A')}")
                print(f"Inactive Layers: {info.get('inactive_layers', 'N/A')}")
                print(f"Progress: {info.get('activation_progress', 'N/A')}")
            except:
                print("âš ï¸  Detailed status not available")
        
        print("="*60 + "\n")
    
    def reset_conversation(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        print("ğŸ”„ Resetting conversation history...")
        self.reset_kv_cache()
        self.history_prompt = ""
        print("âœ… Conversation reset complete")
    
    def chat(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            user_input: ì‚¬ìš©ì ë©”ì‹œì§€
            
        Returns:
            AI ì‘ë‹µ
        """
        # Llama 2 í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì ìš©
        current_prompt = self.history_prompt + f"[INST] {user_input} [/INST]"
        
        # ìƒì„±
        start = time.time()
        outputs = self.llm.generate([current_prompt], self.sampling_params)
        elapsed = time.time() - start
        
        # ê²°ê³¼ ì¶”ì¶œ
        response = outputs[0].outputs[0].text
        
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.history_prompt = current_prompt + f" {response} "
        
        return response, elapsed
    
    def run(self):
        """ì±—ë´‡ ë©”ì¸ ë£¨í”„"""
        print("\n" + "="*60)
        print("Progressive vLLM Chatbot")
        print("="*60)
        print(f"Current Stage: {self.current_stage}")
        print("\nCommands:")
        print("  quit   - Exit chatbot")
        print("  reset  - Reset KV cache and conversation")
        print("  stage2 - Advance to Stage 2")
        print("  stage3 - Advance to Stage 3")
        print("  status - Show model status")
        print("-" * 60)
        
        while True:
            try:
                user_input = input(f"\n[Stage {self.current_stage}] User: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\n\nGoodbye!")
                break
            
            if not user_input:
                continue
            
            # ëª…ë ¹ì–´ ì²˜ë¦¬
            command = user_input.lower()
            
            if command == "quit":
                print("Goodbye!")
                break
            
            elif command == "reset":
                self.reset_conversation()
                continue
            
            elif command == "stage2":
                self.advance_to_stage2()
                continue
            
            elif command == "stage3":
                self.advance_to_stage3()
                continue
            
            elif command == "status":
                self.print_status()
                continue
            
            # ì¼ë°˜ ëŒ€í™”
            response, elapsed = self.chat(user_input)
            
            print(f"Bot: {response}")
            print(f"â±ï¸  Generation time: {elapsed:.2f}s | Stage: {self.current_stage}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì„¤ì • ê²½ë¡œ (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
    model_path = "/acpl-ssd20/1218/A"
    stage2_checkpoint = "/acpl-ssd20/1218/checkpoints/stage2_layers_B.safetensors"
    stage3_checkpoint = "/acpl-ssd20/1218/checkpoints/stage3_layers_C.safetensors"
    
    # ì±—ë´‡ ìƒì„± ë° ì‹¤í–‰
    chatbot = ProgressiveChatbot(
        model_path=model_path,
        stage2_checkpoint=stage2_checkpoint,
        stage3_checkpoint=stage3_checkpoint,
        gpu_memory_utilization=0.9,
        enable_prefix_caching=True,
        enforce_eager=False,  # CUDA Graph ì‚¬ìš© (ë¬¸ì œ ì‹œ Trueë¡œ ë³€ê²½)
    )
    
    chatbot.run()


if __name__ == "__main__":
    main()