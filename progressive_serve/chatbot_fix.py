#!/usr/bin/env python3
"""
Progressive Stage Chatbot with KV Cache Reset
íŒŒì¼ëª…: chatbot.py

ì‚¬ìš©í•˜ëŠ” íŒŒì¼ë“¤:
1. progressive_llama_for_causal_lm_alpha_v0.py - ë©”ì¸ ëª¨ë¸ í´ë˜ìŠ¤
2. progressive_llama_alpha_fixed.py (ë˜ëŠ” progressive_llama_alpha_fixed2.py) - CUDA Graph í˜¸í™˜ ëª¨ë¸
3. alpha_gated_layer.py - Alpha Gating ë ˆì´ì–´

ê¸°ëŠ¥:
- Stage 1/2/3 ì „í™˜ ì§€ì›
- Stage ì „í™˜ ì‹œ KV Cache ì´ˆê¸°í™” í›„ ë§¥ë½ ì¬ê³„ì‚°
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€
- Prefix caching í™œìš©

ëª…ë ¹ì–´:
- quit: ì¢…ë£Œ
- reset: KV cache ì´ˆê¸°í™” ë° ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¦¬ì…‹
- stage2: Stage 2ë¡œ ì „í™˜ (KV cache ì´ˆê¸°í™” í›„ ë§¥ë½ ì¬ê³„ì‚°)
- stage3: Stage 3ë¡œ ì „í™˜ (KV cache ì´ˆê¸°í™” í›„ ë§¥ë½ ì¬ê³„ì‚°)
- status: í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì¶œë ¥
"""

import sys
import os
import time
import torch

# Python path ì„¤ì • - ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ ì¸ì‹
sys.path.insert(0, "/workspace/vllm_test")
sys.path.insert(0, "/acpl-ssd20/1218/A")
sys.path.insert(0, "/home/devewha/Juwon/vllm_test")

# vLLM import
try:
    from vllm import LLM, SamplingParams
    from vllm.model_executor.models.registry import ModelRegistry
    from progressive_serve.progressive_llama_for_causal_lm_alpha_v0 import ProgressiveLlamaForCausalLMAlpha
    print("âœ… vLLM and Custom Model imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import required modules: {e}")
    print("í•„ìš”í•œ íŒŒì¼ë“¤:")
    print("  - progressive_serve/progressive_llama_for_causal_lm_alpha_v0.py")
    print("  - progressive_serve/progressive_llama_alpha_fixed.py")
    print("  - progressive_serve/alpha_gated_layer.py")
    sys.exit(1)

# ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
ModelRegistry.register_model(
    "ProgressiveLlamaForCausalLM",
    ProgressiveLlamaForCausalLMAlpha
)


class ProgressiveChatbot:
    """
    Progressive Stageë¥¼ ì§€ì›í•˜ëŠ” vLLM ì±—ë´‡
    
    Stage ì „í™˜ ì‹œ:
    1. KV Cache ì´ˆê¸°í™” (sleep/wake_up)
    2. ëŒ€í™” ë§¥ë½ ì¬ê³„ì‚°
    """
    
    def __init__(
        self,
        model_path: str,
        stage2_checkpoint: str,
        stage3_checkpoint: str,
        gpu_memory_utilization: float = 0.9,
        enable_prefix_caching: bool = False,  # v0ì—ì„œ multimodal ëª¨ë¸ ë¯¸ì§€ì›ìœ¼ë¡œ ë¹„í™œì„±í™”
        enforce_eager: bool = False,
    ):
        """
        Args:
            model_path: Stage 1 ëª¨ë¸ ê²½ë¡œ
            stage2_checkpoint: Stage 2 weights ê²½ë¡œ
            stage3_checkpoint: Stage 3 weights ê²½ë¡œ
            gpu_memory_utilization: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            enable_prefix_caching: Prefix caching í™œì„±í™”
            enforce_eager: CUDA Graph ë¹„í™œì„±í™” (Trueë©´ ë™ì  ë ˆì´ì–´ ë³€ê²½ ì•ˆì •)
        """
        self.model_path = model_path
        self.stage2_checkpoint = stage2_checkpoint
        self.stage3_checkpoint = stage3_checkpoint
        
        # vLLM ì—”ì§„ ì´ˆê¸°í™”
        print("\n" + "="*60)
        print("Initializing Progressive Chatbot...")
        print("="*60 + "\n")
        
        start_init = time.time()
        
        self.llm = LLM(
            model=model_path,
            trust_remote_code=True,
            gpu_memory_utilization=gpu_memory_utilization,
            enable_prefix_caching=enable_prefix_caching,
            enforce_eager=enforce_eager,
        )
        
        init_time = time.time() - start_init
        print(f"âœ… vLLM Initialization complete: {init_time:.2f}s\n")
        
        # ëª¨ë¸ ê°ì²´ ì ‘ê·¼
        try:
            self.model = self.llm.llm_engine.model_executor.driver_worker.model_runner.model
            print(f"âœ… Model accessed: {type(self.model).__name__}")
        except Exception as e:
            print(f"âŒ Failed to access model: {e}")
            raise
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
        self.sampling_params = SamplingParams(
            temperature=0.7,
            max_tokens=200,
        )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.history_prompt = ""
        
        # í˜„ì¬ Stage
        self.current_stage = 1
        
        print(f"âœ… Chatbot ready at Stage {self.current_stage}")
        print("-" * 60)
    
    def reset_kv_cache(self) -> float:
        """
        KV Cache ì´ˆê¸°í™”
        
        ì—¬ëŸ¬ ë°©ë²• ì‹œë„:
        1. sleep/wake_up (vLLM 0.6+ with enable_sleep_mode)
        2. llm_engine ë‚´ë¶€ scheduler í†µí•œ ì´ˆê¸°í™”
        3. prefix cache ë¦¬ì…‹
        4. ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ (fallback)
        
        Returns:
            ì´ˆê¸°í™”ì— ê±¸ë¦° ì‹œê°„ (ì´ˆ)
        """
        print("ğŸ”„ Resetting KV cache...")
        start = time.time()
        
        success = False
        
        # ë°©ë²• 1: sleep/wake_up (enable_sleep_mode í•„ìš”)
        try:
            if hasattr(self.llm, 'sleep') and hasattr(self.llm, 'wake_up'):
                # sleep modeê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if (hasattr(self.llm, 'llm_engine') and 
                    hasattr(self.llm.llm_engine, 'vllm_config') and
                    hasattr(self.llm.llm_engine.vllm_config, 'model_config') and
                    getattr(self.llm.llm_engine.vllm_config.model_config, 'enable_sleep_mode', False)):
                    self.llm.sleep(level=1)
                    self.llm.wake_up()
                    success = True
                    print("  âœ… Used sleep/wake_up method")
        except Exception as e:
            print(f"  âš ï¸  sleep/wake_up failed: {e}")
        
        # ë°©ë²• 2: Schedulerë¥¼ í†µí•œ KV cache ë¸”ë¡ í•´ì œ
        if not success:
            try:
                engine = self.llm.llm_engine
                
                # Schedulerì˜ block managerë¥¼ í†µí•œ ì´ˆê¸°í™”
                if hasattr(engine, 'scheduler'):
                    schedulers = engine.scheduler
                    if not isinstance(schedulers, list):
                        schedulers = [schedulers]
                    
                    for scheduler in schedulers:
                        # ëª¨ë“  sequence group ì™„ë£Œ ì²˜ë¦¬
                        if hasattr(scheduler, 'running'):
                            scheduler.running.clear()
                        if hasattr(scheduler, 'waiting'):
                            scheduler.waiting.clear()
                        if hasattr(scheduler, 'swapped'):
                            scheduler.swapped.clear()
                        
                        # Block manager free
                        if hasattr(scheduler, 'block_manager'):
                            block_manager = scheduler.block_manager
                            if hasattr(block_manager, 'reset'):
                                block_manager.reset()
                            elif hasattr(block_manager, '_reset'):
                                block_manager._reset()
                    
                    success = True
                    print("  âœ… Used scheduler reset method")
            except Exception as e:
                print(f"  âš ï¸  Scheduler reset failed: {e}")
        
        # ë°©ë²• 3: Prefix cache ë¦¬ì…‹
        if not success:
            try:
                if hasattr(self.llm.llm_engine, 'reset_prefix_cache'):
                    self.llm.llm_engine.reset_prefix_cache()
                    success = True
                    print("  âœ… Used prefix cache reset method")
            except Exception as e:
                print(f"  âš ï¸  Prefix cache reset failed: {e}")
        
        # ë°©ë²• 4: ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ (fallback)
        if not success:
            try:
                import gc
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                print("  âš ï¸  Used fallback (gc + cuda cache clear)")
                print("  â„¹ï¸  Note: KV cache may not be fully cleared, but Stage transition is complete")
                success = True
            except Exception as e:
                print(f"  âŒ Fallback also failed: {e}")
        
        elapsed = time.time() - start
        if success:
            print(f"âœ… KV cache reset complete ({elapsed:.2f}s)")
        else:
            print(f"âš ï¸  KV cache reset may be incomplete ({elapsed:.2f}s)")
        
        return elapsed
    
    def recompute_context(self) -> float:
        """
        í˜„ì¬ ëŒ€í™” ë§¥ë½ì„ ìƒˆ Stageë¡œ ì¬ê³„ì‚°
        
        Stage ì „í™˜ í›„ ê¸°ì¡´ íˆìŠ¤í† ë¦¬ë¡œ ìƒˆë¡œìš´ KV cache ìƒì„±
        
        Returns:
            ì¬ê³„ì‚°ì— ê±¸ë¦° ì‹œê°„ (ì´ˆ)
        """
        if not self.history_prompt:
            print("â„¹ï¸  No history to recompute")
            return 0.0
        
        print("ğŸ”„ Recomputing context with new stage...")
        start = time.time()
        
        # ë¹ˆ ìƒì„± ìš”ì²­ìœ¼ë¡œ KV cacheë§Œ ìƒì„±
        # max_tokens=1ë¡œ ìµœì†Œí•œì˜ ìƒì„±ë§Œ ìˆ˜í–‰
        recompute_params = SamplingParams(
            temperature=0.0,
            max_tokens=1,  # ìµœì†Œ í† í°ë§Œ ìƒì„±
        )
        
        try:
            # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ë¡œ forward pass ìˆ˜í–‰ (KV cache ì¬êµ¬ì¶•)
            _ = self.llm.generate([self.history_prompt], recompute_params)
        except Exception as e:
            print(f"âš ï¸  Context recompute warning: {e}")
        
        elapsed = time.time() - start
        print(f"âœ… Context recomputed ({elapsed:.2f}s)")
        return elapsed
    
    def advance_to_stage2(self) -> bool:
        """
        Stage 2ë¡œ ì „í™˜
        
        1. ìƒˆ ë ˆì´ì–´ weights ë¡œë“œ ë° í™œì„±í™”
        2. KV Cache ì´ˆê¸°í™”
        3. ëŒ€í™” ë§¥ë½ ì¬ê³„ì‚°
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if self.current_stage >= 2:
            print(f"âš ï¸  Already at Stage {self.current_stage}")
            return False
        
        print("\n" + "="*60)
        print("TRANSITIONING TO STAGE 2")
        print("="*60 + "\n")
        
        start = time.time()
        
        try:
            # 1. Stage 2 ë ˆì´ì–´ í™œì„±í™”
            print("[Step 1/3] Activating Stage 2 layers...")
            self.model.advance_to_stage2(layer_b_checkpoint=self.stage2_checkpoint)
            
            # 2. KV Cache ì´ˆê¸°í™”
            print("\n[Step 2/3] Resetting KV cache...")
            self.reset_kv_cache()
            
            # 3. ëŒ€í™” ë§¥ë½ ì¬ê³„ì‚°
            print("\n[Step 3/3] Recomputing conversation context...")
            self.recompute_context()
            
            self.current_stage = 2
            
            total_time = time.time() - start
            print(f"\n{'='*60}")
            print(f"âœ… NOW AT STAGE 2 (Total: {total_time:.2f}s)")
            print(f"{'='*60}\n")
            
            return True
            
        except Exception as e:
            print(f"âŒ Stage 2 transition failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def advance_to_stage3(self) -> bool:
        """
        Stage 3ë¡œ ì „í™˜
        
        1. ìƒˆ ë ˆì´ì–´ weights ë¡œë“œ ë° í™œì„±í™”
        2. KV Cache ì´ˆê¸°í™”
        3. ëŒ€í™” ë§¥ë½ ì¬ê³„ì‚°
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if self.current_stage >= 3:
            print(f"âš ï¸  Already at Stage {self.current_stage}")
            return False
        
        if self.current_stage < 2:
            print("âš ï¸  Must be at Stage 2 first. Advancing to Stage 2...")
            if not self.advance_to_stage2():
                return False
        
        print("\n" + "="*60)
        print("TRANSITIONING TO STAGE 3")
        print("="*60 + "\n")
        
        start = time.time()
        
        try:
            # 1. Stage 3 ë ˆì´ì–´ í™œì„±í™”
            print("[Step 1/3] Activating Stage 3 layers...")
            self.model.advance_to_stage3(layer_c_checkpoint=self.stage3_checkpoint)
            
            # 2. KV Cache ì´ˆê¸°í™”
            print("\n[Step 2/3] Resetting KV cache...")
            self.reset_kv_cache()
            
            # 3. ëŒ€í™” ë§¥ë½ ì¬ê³„ì‚°
            print("\n[Step 3/3] Recomputing conversation context...")
            self.recompute_context()
            
            self.current_stage = 3
            
            total_time = time.time() - start
            print(f"\n{'='*60}")
            print(f"âœ… NOW AT STAGE 3 - FULL MODEL (Total: {total_time:.2f}s)")
            print(f"{'='*60}\n")
            
            return True
            
        except Exception as e:
            print(f"âŒ Stage 3 transition failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def print_status(self):
        """í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì¶œë ¥"""
        print("\n" + "="*60)
        print(f"CHATBOT STATUS")
        print("="*60)
        print(f"Current Stage: {self.current_stage}")
        print(f"History Length: {len(self.history_prompt)} chars")
        
        try:
            self.model.print_status()
        except:
            try:
                info = self.model.get_stage_info()
                print(f"Active Layers: {info.get('active_layers', 'N/A')}")
                print(f"Inactive Layers: {info.get('inactive_layers', 'N/A')}")
                print(f"Progress: {info.get('activation_progress', 'N/A')}")
            except:
                print("âš ï¸  Detailed status not available")
        
        print("="*60 + "\n")
    
    def reset_conversation(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        print("ğŸ”„ Resetting conversation history...")
        self.reset_kv_cache()
        self.history_prompt = ""
        print("âœ… Conversation reset complete")
    
    def chat(self, user_input: str) -> tuple:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            user_input: ì‚¬ìš©ì ë©”ì‹œì§€
            
        Returns:
            AI ì‘ë‹µ
        """
        # Llama 2 í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì ìš©
        current_prompt = self.history_prompt + f"[INST] {user_input} [/INST]"
        
        # ìƒì„±
        start = time.time()
        outputs = self.llm.generate([current_prompt], self.sampling_params)
        elapsed = time.time() - start
        
        # ê²°ê³¼ ì¶”ì¶œ
        response = outputs[0].outputs[0].text
        
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.history_prompt = current_prompt + f" {response} "
        
        return response, elapsed
    
    def run(self):
        """ì±—ë´‡ ë©”ì¸ ë£¨í”„"""
        print("\n" + "="*60)
        print("Progressive vLLM Chatbot")
        print("="*60)
        print(f"Current Stage: {self.current_stage}")
        print("\nCommands:")
        print("  quit   - Exit chatbot")
        print("  reset  - Reset KV cache and conversation")
        print("  stage2 - Advance to Stage 2")
        print("  stage3 - Advance to Stage 3")
        print("  status - Show model status")
        print("-" * 60)
        
        while True:
            try:
                user_input = input(f"\n[Stage {self.current_stage}] User: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\n\nGoodbye!")
                break
            
            if not user_input:
                continue
            
            # ëª…ë ¹ì–´ ì²˜ë¦¬
            command = user_input.lower()
            
            if command == "quit":
                print("Goodbye!")
                break
            
            elif command == "reset":
                self.reset_conversation()
                continue
            
            elif command == "stage2":
                self.advance_to_stage2()
                continue
            
            elif command == "stage3":
                self.advance_to_stage3()
                continue
            
            elif command == "status":
                self.print_status()
                continue
            
            # ì¼ë°˜ ëŒ€í™”
            response, elapsed = self.chat(user_input)
            
            print(f"Bot: {response}")
            print(f"â±ï¸  Generation time: {elapsed:.2f}s | Stage: {self.current_stage}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì„¤ì • ê²½ë¡œ (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
    model_path = "/acpl-ssd20/1218/A"
    stage2_checkpoint = "/acpl-ssd20/1218/checkpoints/stage2_layers_B.safetensors"
    stage3_checkpoint = "/acpl-ssd20/1218/checkpoints/stage3_layers_C.safetensors"
    
    # ì±—ë´‡ ìƒì„± ë° ì‹¤í–‰
    chatbot = ProgressiveChatbot(
        model_path=model_path,
        stage2_checkpoint=stage2_checkpoint,
        stage3_checkpoint=stage3_checkpoint,
        gpu_memory_utilization=0.9,
        enable_prefix_caching=False,  # v0ì—ì„œ multimodal ëª¨ë¸ ë¯¸ì§€ì›ìœ¼ë¡œ ë¹„í™œì„±í™”
        enforce_eager=False,  # CUDA Graph ì‚¬ìš© (ë¬¸ì œ ì‹œ Trueë¡œ ë³€ê²½)
    )
    
    chatbot.run()


if __name__ == "__main__":
    main()