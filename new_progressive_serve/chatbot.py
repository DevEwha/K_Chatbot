#!/usr/bin/env python3
"""
ì‹¤í–‰: python chatbot_fixed.py
ëŒ€í™”í˜• Progressive Stage í…ŒìŠ¤íŠ¸ (Interactive Version)
íŒŒì¼ëª…: chatbot_fixed.py

âœ… vLLM 0.7.4 í˜¸í™˜ ìˆ˜ì •ì‚¬í•­:
1. sleep/wake ì œê±° (vLLM 0.7.4ì—ì„œ ë¯¸ì§€ì›)
2. KV Cache ê´€ë¦¬ë¥¼ ìœ„í•œ ëŒ€ì²´ ë°©ë²• êµ¬í˜„
3. Partial KV Cache Reuse ì§€ì›
4. Stage ì „í™˜ ì‹œ cache hint í™œìš©
"""

import sys
import os
import time
import torch

# [í•„ìˆ˜] Python path ì„¤ì • - ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œë¥¼ ì¸ì‹í•˜ê²Œ í•©ë‹ˆë‹¤.
sys.path.insert(0, "/workspace/vllm_test")
sys.path.insert(0, "/acpl-ssd20/1218/A")
sys.path.insert(0, "/home/devewha/Juwon/vllm_test")
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))  # í˜„ì¬ ë””ë ‰í† ë¦¬

# vLLM import
try:
    from vllm import LLM, SamplingParams
    from vllm.model_executor.models.registry import ModelRegistry
    print("âœ… vLLM imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import vLLM: {e}")
    sys.exit(1)

# Custom model import
try:
    from progressive_llama_for_causal_lm_alpha_v0 import ProgressiveLlamaForCausalLMAlpha
    print("âœ… Custom Model imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import custom model: {e}")
    # Fallback to original
    try:
        from progressive_llama_for_causal_lm_alpha_v0 import ProgressiveLlamaForCausalLMAlpha
        print("âš ï¸  Using original model (not fixed version)")
    except ImportError:
        print(f"âŒ Failed to import any custom model")
        sys.exit(1)

# [í•„ìˆ˜] ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
ModelRegistry.register_model(
    "ProgressiveLlamaForCausalLM",
    ProgressiveLlamaForCausalLMAlpha
)


class ProgressiveChatbot:
    """ëŒ€í™”í˜• Progressive Stage ì±—ë´‡ (vLLM 0.7.4 í˜¸í™˜)"""
    
    def __init__(self, model_path, stage2_path, stage3_path):
        self.model_path = model_path
        self.stage2_path = stage2_path
        self.stage3_path = stage3_path
        self.current_stage = 1
        self.conversation_history = ""
        self.turn_count = 0
        
        # í†µê³„ ì •ë³´
        self.stage_stats = {
            1: {"inference_times": [], "token_counts": []},
            2: {"inference_times": [], "token_counts": []},
            3: {"inference_times": [], "token_counts": []}
        }
        
        self.llm = None
        self.model = None
        self.sampling_params = None
        
        # Cache hint ì €ì¥
        self._last_cache_hint = None
        
    def initialize(self):
        """vLLM ì—”ì§„ ì´ˆê¸°í™”"""
        print("\n" + "="*80)
        print("ğŸš€ Progressive LLM Chatbot - Initialization (vLLM 0.7.4)")
        print("="*80 + "\n")
        
        print("â³ Initializing vLLM with Prefix Caching enabled...")
        start_init = time.time()
        
        try:
            self.llm = LLM(
                model=self.model_path,
                trust_remote_code=True,
                gpu_memory_utilization=0.9,
                enforce_eager=False,
                enable_prefix_caching=True  # Prefix caching í™œì„±í™”!
            )
        except Exception as e:
            print(f"âŒ Failed to initialize vLLM: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
        
        init_time = time.time() - start_init
        print(f"âœ… Initialization complete: {init_time:.2f}s\n")
        
        # ëª¨ë¸ ê°ì²´ ì§ì ‘ ì ‘ê·¼
        try:
            self.model = self.llm.llm_engine.model_executor.driver_worker.model_runner.model
            print(f"âœ… Model accessed: {type(self.model).__name__}")
        except Exception as e:
            print(f"âŒ Failed to access model: {e}")
            sys.exit(1)
        
        # Sampling íŒŒë¼ë¯¸í„° ì„¤ì •
        self.sampling_params = SamplingParams(
            max_tokens=100,
            temperature=0.7,
            top_p=0.95,
        )
        
        print(f"âœ… Sampling parameters configured")
        print(f"   - max_tokens: 100")
        print(f"   - temperature: 0.7")
        print(f"   - top_p: 0.95")
        print(f"\nğŸ¯ Currently in Stage {self.current_stage}\n")
    
    def _clear_kv_cache_v074(self):
        """
        vLLM 0.7.4ìš© KV Cache ì´ˆê¸°í™”
        
        Note: sleep/wakeëŠ” vLLM 0.7.4ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ
        ëŒ€ì•ˆ: ìƒˆ ìš”ì²­ì„ ë³´ë‚´ë©´ ìë™ìœ¼ë¡œ prefixê°€ ë¬´íš¨í™”ë¨
        """
        print("ğŸ§¹ Invalidating KV Cache (vLLM 0.7.4 method)...")
        
        try:
            # vLLM 0.7.4ì—ì„œëŠ” schedulerë¥¼ í†µí•´ cacheë¥¼ ê´€ë¦¬
            scheduler = self.llm.llm_engine.scheduler
            
            # ëª¨ë“  seq_group ì œê±°
            if hasattr(scheduler, 'abort_seq_group'):
                # í™œì„± ìš”ì²­ ì¤‘ë‹¨
                for seq_group in list(scheduler.running):
                    scheduler.abort_seq_group(seq_group.request_id)
                for seq_group in list(scheduler.waiting):
                    scheduler.abort_seq_group(seq_group.request_id)
                print("  âœ… Aborted all active sequences")
        except Exception as e:
            print(f"  âš ï¸  Cache management warning: {e}")
        
        # prefix cachingì´ í™œì„±í™”ëœ ê²½ìš°, ìƒˆ ìš”ì²­ ì‹œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
        print("  âœ… Cache will be refreshed on next request")
    
    def _partial_invalidate_cache(self, from_layer: int):
        """
        Partial KV Cache Invalidation (PDFì˜ í•µì‹¬ ì•„ì´ë””ì–´)
        
        Note: vLLM 0.7.4ì—ì„œëŠ” layer-wise invalidationì´ ì§ì ‘ ì§€ì›ë˜ì§€ ì•ŠìŒ
        ì´ ë©”ì„œë“œëŠ” í–¥í›„ vLLM í™•ì¥ì„ ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ ì˜ˆì‹œ
        
        Args:
            from_layer: ì´ ë ˆì´ì–´ë¶€í„° cache ë¬´íš¨í™”
        """
        print(f"ğŸ“Š Partial Cache Hint: Keep layers 0-{from_layer-1}, invalidate {from_layer}+")
        
        # TODO: vLLM cache engine í™•ì¥ ì‹œ êµ¬í˜„
        # í˜„ì¬ëŠ” hintë§Œ ê¸°ë¡í•˜ê³  ì „ì²´ ìºì‹œë¥¼ ìƒˆë¡œ ê³„ì‚°
        
        # ì‹¤ì œ êµ¬í˜„ ì‹œ:
        # self.llm.llm_engine.cache_engine.partial_invalidate(from_layer)
        
        self._last_cache_hint = {
            "keep_prefix_layers": from_layer,
            "recompute_from_layer": from_layer,
            "timestamp": time.time(),
        }
        
        print(f"  âœ… Cache hint recorded (partial reuse possible: {from_layer} layers)")
        
    def advance_stage(self, target_stage):
        """
        Stage ì „í™˜ (vLLM 0.7.4 í˜¸í™˜)
        
        âœ… ìˆ˜ì •ì‚¬í•­:
        - sleep/wake ì œê±°
        - cache hint í™œìš©
        - partial invalidation ì§€ì›
        """
        if target_stage == self.current_stage:
            print(f"âš ï¸  Already in Stage {target_stage}")
            return False
        
        if target_stage < self.current_stage:
            print(f"âš ï¸  Cannot downgrade from Stage {self.current_stage} to Stage {target_stage}")
            return False
        
        if target_stage > 3:
            print(f"âš ï¸  Invalid stage: {target_stage}. Valid stages: 1, 2, 3")
            return False
        
        print(f"\n{'='*80}")
        print(f"ğŸ”„ Stage Transition: {self.current_stage} â†’ {target_stage}")
        print(f"{'='*80}\n")
        
        # 1. Stage ì „í™˜ with cache hint
        start_transition = time.time()
        cache_hint = None
        
        try:
            if target_stage == 2:
                print(f"ğŸ“¦ Loading Stage 2 layers from: {self.stage2_path}")
                
                # cache hint ë°˜í™˜ ì§€ì› í™•ì¸
                if hasattr(self.model, 'advance_to_stage2'):
                    # ìˆ˜ì •ëœ ë²„ì „: return_cache_hint=True
                    try:
                        cache_hint = self.model.advance_to_stage2(
                            layer_b_checkpoint=self.stage2_path,
                            return_cache_hint=True
                        )
                    except TypeError:
                        # ì´ì „ ë²„ì „: return_cache_hint ë¯¸ì§€ì›
                        self.model.advance_to_stage2(
                            layer_b_checkpoint=self.stage2_path
                        )
                        # get_last_cache_hint ì‹œë„
                        if hasattr(self.model, 'get_last_cache_hint'):
                            cache_hint = self.model.get_last_cache_hint()
                        
            elif target_stage == 3:
                if self.current_stage == 1:
                    print("âš ï¸  Must advance to Stage 2 first")
                    return False
                    
                print(f"ğŸ“¦ Loading Stage 3 layers from: {self.stage3_path}")
                
                if hasattr(self.model, 'advance_to_stage3'):
                    try:
                        cache_hint = self.model.advance_to_stage3(
                            layer_c_checkpoint=self.stage3_path,
                            return_cache_hint=True
                        )
                    except TypeError:
                        self.model.advance_to_stage3(
                            layer_c_checkpoint=self.stage3_path
                        )
                        if hasattr(self.model, 'get_last_cache_hint'):
                            cache_hint = self.model.get_last_cache_hint()
            
            transition_time = time.time() - start_transition
            print(f"âœ… Stage {target_stage} transition complete: {transition_time:.2f}s")
            
        except Exception as e:
            print(f"âŒ Stage transition failed: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        # 2. Cache ì²˜ë¦¬
        if cache_hint:
            # Partial KV Cache Reuse (PDFì˜ í•µì‹¬!)
            recompute_from = cache_hint.get('recompute_from_layer', 0)
            total_layers = cache_hint.get('total_layers', 32)
            reuse_ratio = cache_hint.get('cache_reuse_ratio', 0)
            
            print(f"\nğŸ“Š Partial KV Cache Reuse Analysis:")
            print(f"   - Keep layers: 0 ~ {recompute_from - 1}")
            print(f"   - Recompute from: Layer {recompute_from}")
            print(f"   - Cache reuse ratio: {reuse_ratio:.1f}%")
            
            # Partial invalidation ì‹œë„
            self._partial_invalidate_cache(recompute_from)
        else:
            # ì „ì²´ ìºì‹œ ë¬´íš¨í™” (fallback)
            print("\nâš ï¸  No cache hint available, full cache invalidation")
            self._clear_kv_cache_v074()
        
        self.current_stage = target_stage
        
        # 3. ìƒíƒœ í™•ì¸
        try:
            self.model.print_status()
        except:
            try:
                info = self.model.get_stage_info()
                print(f"   Current stage: {info.get('stage')}")
            except:
                pass
        
        print(f"\nğŸ¯ Now in Stage {self.current_stage}\n")
        return True
    
    def generate_response(self, user_input):
        """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        self.turn_count += 1
        
        # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if self.conversation_history:
            full_prompt = self.conversation_history + f"\nUser: {user_input}\nAssistant:"
        else:
            full_prompt = f"User: {user_input}\nAssistant:"
        
        # ì¶”ë¡  ì‹¤í–‰
        print(f"\nğŸ’­ Thinking... (Stage {self.current_stage})")
        start_time = time.time()
        
        try:
            outputs = self.llm.generate([full_prompt], self.sampling_params)
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            return None
        
        elapsed_time = time.time() - start_time
        
        # ì‘ë‹µ ì¶”ì¶œ
        response = outputs[0].outputs[0].text.strip()
        token_count = len(outputs[0].outputs[0].token_ids)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stage_stats[self.current_stage]["inference_times"].append(elapsed_time)
        self.stage_stats[self.current_stage]["token_counts"].append(token_count)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.conversation_history = full_prompt + " " + response
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*80}")
        print(f"ğŸ¤– Assistant (Stage {self.current_stage} - Turn {self.turn_count})")
        print(f"{'='*80}")
        print(f"{response}")
        print(f"{'-'*80}")
        print(f"â±ï¸  Inference time: {elapsed_time:.4f}s")
        print(f"ğŸ”¢ Generated tokens: {token_count}")
        print(f"ğŸ“ Context length: {len(full_prompt.split())} words")
        print(f"{'='*80}\n")
        
        return response
    
    def print_statistics(self):
        """ëŒ€í™” í†µê³„ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print("ğŸ“Š Session Statistics")
        print(f"{'='*80}")
        print(f"Total turns: {self.turn_count}")
        print(f"Final stage: {self.current_stage}")
        print(f"Context length: {len(self.conversation_history.split())} words\n")
        
        for stage in [1, 2, 3]:
            times = self.stage_stats[stage]["inference_times"]
            tokens = self.stage_stats[stage]["token_counts"]
            
            if times:
                avg_time = sum(times) / len(times)
                avg_tokens = sum(tokens) / len(tokens)
                print(f"Stage {stage}:")
                print(f"  - Turns: {len(times)}")
                print(f"  - Avg inference time: {avg_time:.4f}s")
                print(f"  - Avg tokens generated: {avg_tokens:.1f}")
                print(f"  - Total time: {sum(times):.4f}s")
        
        # Cache hint ì •ë³´
        if self._last_cache_hint:
            print(f"\nLast Cache Hint:")
            print(f"  - Keep prefix layers: {self._last_cache_hint.get('keep_prefix_layers', 'N/A')}")
            print(f"  - Recompute from: {self._last_cache_hint.get('recompute_from_layer', 'N/A')}")
        
        print(f"{'='*80}\n")
    
    def print_help(self):
        """ë„ì›€ë§ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print("ğŸ“– Available Commands")
        print(f"{'='*80}")
        print("/stage2    - Advance to Stage 2 (load B layers)")
        print("/stage3    - Advance to Stage 3 (load C layers)")
        print("/stats     - Show session statistics")
        print("/clear     - Clear conversation history")
        print("/status    - Show model status")
        print("/cache     - Show cache hint info")
        print("/help      - Show this help message")
        print("/exit      - Exit the chatbot")
        print("\nOr just type your message to chat!")
        print(f"{'='*80}\n")
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = ""
        print("âœ… Conversation history cleared")
        
        # ìºì‹œë„ ì´ˆê¸°í™”
        self._clear_kv_cache_v074()
        print()
    
    def show_model_status(self):
        """ëª¨ë¸ ìƒíƒœ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print("ğŸ“Š Model Status")
        print(f"{'='*80}")
        
        try:
            if hasattr(self.model, 'print_status'):
                self.model.print_status()
            elif hasattr(self.model, 'get_stage_info'):
                info = self.model.get_stage_info()
                print(f"Current Stage: {info.get('stage', 'N/A')}")
                print(f"Active Layers: {info.get('active_layers', 'N/A')}")
                print(f"Inactive Layers: {info.get('inactive_layers', 'N/A')}")
                print(f"Progress: {info.get('activation_progress', 'N/A')}")
            else:
                print(f"Current Stage: {self.current_stage}")
        except Exception as e:
            print(f"âš ï¸  Could not get model status: {e}")
        
        print(f"{'='*80}\n")
    
    def show_cache_info(self):
        """Cache ì •ë³´ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print("ğŸ“Š Cache Information")
        print(f"{'='*80}")
        
        if self._last_cache_hint:
            print(f"Last Cache Hint:")
            for key, value in self._last_cache_hint.items():
                print(f"  - {key}: {value}")
        else:
            print("No cache hint available")
        
        # ëª¨ë¸ì˜ cache hintë„ í™•ì¸
        try:
            if hasattr(self.model, 'get_last_cache_hint'):
                model_hint = self.model.get_last_cache_hint()
                if model_hint:
                    print(f"\nModel's Cache Hint:")
                    for key, value in model_hint.items():
                        print(f"  - {key}: {value}")
        except:
            pass
        
        print(f"{'='*80}\n")
    
    def run(self):
        """ëŒ€í™”í˜• ë£¨í”„ ì‹¤í–‰"""
        self.initialize()
        self.print_help()
        
        print("ğŸ’¬ Chat started! Type your message or use commands.\n")
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
                user_input = input(f"[Stage {self.current_stage}] You: ").strip()
                
                if not user_input:
                    continue
                
                # ëª…ë ¹ì–´ ì²˜ë¦¬
                if user_input.startswith("/"):
                    command = user_input.lower()
                    
                    if command == "/exit" or command == "/quit":
                        print("\nğŸ‘‹ Goodbye!")
                        self.print_statistics()
                        break
                    
                    elif command == "/stage2":
                        self.advance_stage(2)
                    
                    elif command == "/stage3":
                        self.advance_stage(3)
                    
                    elif command == "/stats":
                        self.print_statistics()
                    
                    elif command == "/clear":
                        self.clear_history()
                    
                    elif command == "/status":
                        self.show_model_status()
                    
                    elif command == "/cache":
                        self.show_cache_info()
                    
                    elif command == "/help":
                        self.print_help()
                    
                    else:
                        print(f"âš ï¸  Unknown command: {user_input}")
                        print("Type /help to see available commands\n")
                
                # ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
                else:
                    self.generate_response(user_input)
            
            except KeyboardInterrupt:
                print("\n\nâš ï¸  Interrupted by user")
                self.print_statistics()
                break
            
            except Exception as e:
                print(f"âŒ Error: {e}")
                import traceback
                traceback.print_exc()


def main():
    # ì„¤ì • ê²½ë¡œ
    model_path = "/acpl-ssd20/1218/A"
    stage2_path = "/acpl-ssd20/1218/checkpoints/stage2_layers_B.safetensors"
    stage3_path = "/acpl-ssd20/1218/checkpoints/stage3_layers_C.safetensors"
    
    # ì±—ë´‡ ìƒì„± ë° ì‹¤í–‰
    chatbot = ProgressiveChatbot(
        model_path=model_path,
        stage2_path=stage2_path,
        stage3_path=stage3_path
    )
    
    chatbot.run()


if __name__ == "__main__":
    main()
